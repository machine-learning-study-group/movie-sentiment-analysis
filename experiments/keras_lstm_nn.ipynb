{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/michael/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# uncomment these for Google collab, will have already been installed in local environment \n",
    "# if 'pip install -r requirements.txt' has been run\n",
    "#!pip install nltk\n",
    "#!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "if not os.path.isdir('../aclImdb'):\n",
    "    if not os.path.isfile('../aclImdb_v1.tar.gz'):\n",
    "      !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "    if not os.path.isdir('../aclImdb'):  \n",
    "      !tar -xf aclImdb_v1.tar.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=3000\n",
    "# SAMPLE_SIZE=12500\n",
    "positive_sample_file_list = glob.glob(os.path.join('../aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('../aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): There are few really hilarious films about science fiction but this one will knock your sox off. The\n",
      "Negative review(s): I can find very little thats good to say about this film. I am sure the idea and script looked good \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1][:100])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1][:100])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjDcOMEUoToE"
   },
   "source": [
    "### LSTM with Keras (Sequential model)\n",
    "\n",
    "\n",
    "Please note that the below code is executed on GPU instances on Colab, this wont work on your local machine, use the flag to enable/disable running in CPU or GPU mode, set `run_in_GPU_mode_on_colab=false` in order to be able to run in CPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# def lstm_keras():\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# def lstm_keras():\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "vocab_size = 1000\n",
    "\n",
    "# Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\\\n",
    "#           lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "tokenize = Tokenizer(num_words=vocab_size)\n",
    "tokenize.fit_on_texts(X_train)\n",
    "\n",
    "encoded_X_train = tokenize.texts_to_sequences(X_train)\n",
    "encoded_X_test = tokenize.texts_to_sequences(X_test)\n",
    "\n",
    "encoded_X_train = sequence.pad_sequences(encoded_X_train, maxlen=vocab_size)\n",
    "encoded_X_test = sequence.pad_sequences(encoded_X_test, maxlen=vocab_size)\n",
    "\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(y_train)\n",
    "encoded_y_train = encoder.transform(y_train)\n",
    "encoded_y_test = encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "fVN5C_34oSgO",
    "outputId": "03f65fd1-c9fb-47dc-8025-afa10d4f68d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1000, 512)         512000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               328192    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 840,321\n",
      "Trainable params: 840,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 16875 samples, validate on 1875 samples\n",
      "Epoch 1/1\n",
      "16875/16875 [==============================] - 270s 16ms/step - loss: 0.4722 - acc: 0.7761 - val_loss: 0.3922 - val_acc: 0.8341\n",
      "6250/6250 [==============================] - 27s 4ms/step\n",
      "Test score: 0.38922087396144867\n",
      "Test accuracy: 0.83216\n"
     ]
    }
   ],
   "source": [
    "max_features = 1000\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 512,input_length=vocab_size))\n",
    "model.add(LSTM(128))  # try using a GRU instead, for fun\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "           optimizer='adam',\n",
    "           metrics=['accuracy'])\n",
    "\n",
    "batch_size=64\n",
    "epochs=3\n",
    "history = model.fit(encoded_X_train, encoded_y_train, \n",
    "              batch_size=batch_size, \n",
    "              epochs=epochs, \n",
    "              verbose=1, \n",
    "              validation_split=0.1)\n",
    "\n",
    "score = model.evaluate(encoded_X_test, encoded_y_test, \n",
    "                     batch_size=batch_size, verbose=1)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjDcOMEUoToE"
   },
   "source": [
    "### run with GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_keras():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "    vocab_size = 1000\n",
    "\n",
    "    # Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',\\\n",
    "    #           lower=True, split=' ', char_level=False, oov_token=None, document_count=0)\n",
    "\n",
    "    tokenize = Tokenizer(num_words=vocab_size)\n",
    "    tokenize.fit_on_texts(X_train)\n",
    "\n",
    "    encoded_X_train = tokenize.texts_to_sequences(X_train)\n",
    "    encoded_X_test = tokenize.texts_to_sequences(X_test)\n",
    "\n",
    "    encoded_X_train = sequence.pad_sequences(encoded_X_train, maxlen=vocab_size)\n",
    "    encoded_X_test = sequence.pad_sequences(encoded_X_test, maxlen=vocab_size)\n",
    "\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(y_train)\n",
    "    encoded_y_train = encoder.transform(y_train)\n",
    "    encoded_y_test = encoder.transform(y_test)\n",
    "\n",
    "    max_features = 1000\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, 512,input_length=vocab_size))\n",
    "    model.add(LSTM(128))  # try using a GRU instead, for fun\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "    batch_size=64\n",
    "    epochs=3\n",
    "\n",
    "    history = model.fit(encoded_X_train, encoded_y_train, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  verbose=1, \n",
    "                  validation_split=0.1)\n",
    "    \n",
    "    score = model.evaluate(encoded_X_test, encoded_y_test,batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    score_tensor  = tf.Variable(score, tf.float32, name=\"score_tensor\")\n",
    "    return score_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 1000, 512)         512000    \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 128)               328192    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 840,321\n",
      "Trainable params: 840,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 4050 samples, validate on 450 samples\n",
      "Epoch 1/3\n",
      "4050/4050 [==============================] - 67s 16ms/step - loss: 0.6688 - acc: 0.6086 - val_loss: 0.6003 - val_acc: 0.6356\n",
      "Epoch 2/3\n",
      "4050/4050 [==============================] - 65s 16ms/step - loss: 0.5027 - acc: 0.7610 - val_loss: 0.5616 - val_acc: 0.7067\n",
      "Epoch 3/3\n",
      "4050/4050 [==============================] - 65s 16ms/step - loss: 0.3847 - acc: 0.8309 - val_loss: 0.4643 - val_acc: 0.7756\n",
      "1500/1500 [==============================] - 7s 4ms/step\n",
      "Test score: 0.44715462438265485\n",
      "Test accuracy: 0.7813333338101704\n",
      "Duration on the GPU: 0.09572267532348633 seconds\n"
     ]
    }
   ],
   "source": [
    "global history\n",
    "\n",
    "gpu_device_name = tf.test.gpu_device_name()\n",
    "run_on_GPU = 'gpu' in gpu_device_name.lower() if gpu_device_name else False\n",
    "\n",
    "if run_on_GPU:\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "\n",
    "    with tf.device('/gpu:0'):\n",
    "        print('Running on GPU')\n",
    "        result = lstm_keras()\n",
    "        session_gpu = tf.Session(config=config)\n",
    "        session_gpu.run(tf.tables_initializer())\n",
    "        session_gpu.run(tf.global_variables_initializer())\n",
    "        session_gpu.run(tf.local_variables_initializer())        \n",
    "        start = time.time()\n",
    "        session_gpu.run(result)\n",
    "        end = time.time()\n",
    "        gpu_time = end - start\n",
    "        print('Duration on the GPU: {} seconds'.format(gpu_time))\n",
    "else:\n",
    "    start = time.time()\n",
    "    lstm_keras()\n",
    "    end = time.time()\n",
    "    cpu_time = end - start\n",
    "    print('Duration on the CPU: {} seconds'.format(cpu_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   val_loss   val_acc      loss       acc          title  sample_size  \\\n",
      "0  0.547166  0.802222  0.322978  0.860494  Keras LSTM NN         3000   \n",
      "\n",
      "   nb_epochs  \n",
      "0          3  \n"
     ]
    }
   ],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "history_df = history_df[history_df['val_acc']==history_df.val_acc.max()]\n",
    "history_df.reset_index(inplace=True)\n",
    "history_df[\"title\"]=[\"Keras LSTM NN\"]\n",
    "history_df[\"sample_size\"]=[SAMPLE_SIZE]\n",
    "history_df[\"nb_epochs\"]=epochs\n",
    "history_df.drop(labels=\"index\",axis=1,inplace=True)\n",
    "print(history_df)\n",
    "history_df.to_csv(path_or_buf=history_df.iloc[0].title+\".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment analysis of movies (IMDB).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "mlsg",
   "language": "python",
   "name": "mlsg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
