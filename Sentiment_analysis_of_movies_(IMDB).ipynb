{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0XMzfB5_EtE"
   },
   "source": [
    "### Sentiment analysis of movie (IMDB) reviews using dataset provided by the ACL 2011 paper, see http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "#### Dataset can be downloaded separately from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, but wont be necessary as the download process has been embedded in the notebook and source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.11.0)\n",
      "Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.14.6)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (0.19.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.11.0)\n",
      "Requirement already satisfied, skipping upgrade: bz2file in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (0.98)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.9.35)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.18.4)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.35 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.12.35)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.3)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.2.0,>=0.1.10 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.1.13)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.22)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.6)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2018.10.15)\n",
      "Requirement already satisfied, skipping upgrade: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.35->boto3->smart-open>=1.2.1->gensim) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.35->boto3->smart-open>=1.2.1->gensim) (2.5.3)\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On the MacOSX, you will need to install wget, see https://www.mkyong.com/mac/wget-on-mac-os-x/\n"
     ]
    }
   ],
   "source": [
    "# MacOSX: See https://www.mkyong.com/mac/wget-on-mac-os-x/ for wget\n",
    "print('On the MacOSX, you will need to install wget, see https://www.mkyong.com/mac/wget-on-mac-os-x/')\n",
    "\n",
    "if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "  !wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz \n",
    "\n",
    "if not os.path.isfile('aclImdb'):  \n",
    "  !tar -xf aclImdb_v1.tar.gz \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=600\n",
    "positive_sample_file_list = glob.glob(os.path.join('aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMYBkcdIB9uc"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cz5eJi7AGSqR"
   },
   "outputs": [],
   "source": [
    "positive_strings = [load_doc(x) for x in positive_sample_file_list]\n",
    "#print('\\n Positive reviews \\n ',positive_strings[:5])\n",
    "\n",
    "negative_strings = [load_doc(x) for x in negative_sample_file_list]\n",
    "#print('\\n Negative reviews \\n ', negative_strings[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0WiHTr7I4CN"
   },
   "outputs": [],
   "source": [
    "positive_tokenized = [word_tokenize(s) for s in positive_strings]\n",
    "#print('\\n Positive tokenized 1 \\n {} \\n\\n Positive tokenized 2 \\n {}'. format(positive_tokenized[1], positive_tokenized[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDP-eqAGIq5R"
   },
   "outputs": [],
   "source": [
    "negative_tokenized = [word_tokenize(s) for s in negative_strings]\n",
    "#print('\\n Negative tokenized 1 \\n {} \\n\\n  Negative tokenized 2 \\n {}'. format(negative_tokenized[1], negative_tokenized[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "6bgN1KJRMPpq",
    "outputId": "aa69f5b0-246f-4805-bc36-9944c1f36b3a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: >\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 161165\n",
      "Non alphanumeric characters found in universe vocabulary {'=', ')', ';', '[', '-', \"'\", ':', '(', '}', '!', '?', ']'}\n",
      "Word count across all reviews (after stripping tokens): 139772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 270565\n",
      "Non alphanumeric characters found in universe vocabulary {\"'\", ')', '?', '=', ';', '[', '-', ']', '!', '}', '(', ':'}\n",
      "Word count across all reviews (after stripping tokens): 234529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <modified: >\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "with open('aclImdb/imdb.vocab', encoding='utf8') as f:\n",
    "    #content = f.readlines()\n",
    "    universe_vocabulary = [x.strip() for x in f.readlines()]\n",
    "\n",
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "\n",
    "#Checking the not alphanumeric characters in vocabulary\n",
    "non_alphanumeric_set = set()\n",
    "for word in universe_vocabulary:\n",
    "    non_alphanumeric_set |= set(re.findall('\\W', word))\n",
    "print('Non alphanumeric characters found in universe vocabulary', non_alphanumeric_set)\n",
    "\n",
    "\n",
    "stripped_positive_tokenized = []\n",
    "for tokens in positive_tokenized:\n",
    "  stripped_positive_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_positive_tokenized]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "DSFWrZInMueS",
    "outputId": "4ae1eab3-3e09-41a5-9778-22aa72ce5cdb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<<<<<<< local <modified: >\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 161165\n",
      "Word count across all reviews (after stripping tokens): 135014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "=======\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 270565\n",
      "Word count across all reviews (after stripping tokens): 243514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">>>>>>> remote <modified: >\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "stripped_negative_tokenized = []\n",
    "for tokens in negative_tokenized:\n",
    "  stripped_negative_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_negative_tokenized]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnu21deYOkDE"
   },
   "source": [
    "## Modelling \n",
    "\n",
    "We have decided to do the use the below models and vectorisation techniques to test our their accuracy / score, the idea is to use a one model and one vectorization technique and plot a score.\n",
    "\n",
    "**Simple models**\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forst\n",
    "- LSTM\n",
    "- GRU\n",
    "- CNN\n",
    "\n",
    "**Vectorisation techniques**\n",
    "- Bag of Words\n",
    "- Word2Vec\n",
    "- TFIDF (probability scores)\n",
    "- FastText\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGh1Vqp3XNJI"
   },
   "source": [
    "## Logistic Regression.\n",
    "## Introducing Pipeline: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "## Introducing TfdfVectorizer: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "## Introducing cross_val_score http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): I thought this was a splendid showcase for Mandy's bodacious bod. If you don't expect anything else, such as clever plot twists and believable character development, you won't be disappointed. Consider this a Sports Illustrated shoot whose character goes around killing people, especially those who threaten to come between her and her 'Mommy' (Suzanna Arquette, who obviously doesn't want to play the sex kitten - she leaves that up to her daughter).  Mandy's face is a little too perfect, but her body is a complete 5-alarm fire, up there in the ranks of Sophia Loren when it comes to natural bustiness, a perfect 7-to-10 ratio of waist to hips, and splendidly configured legs, right down to her feet. (There has to be some ideal configuration of thighs to knees to calves to ankles that is altogether pleasing to the eye; Mandy certainly is the model for this idealized ratio).  And no flat butt to boot, which seems to be the undoing of many a busty babe with curves everywhere except in the 'nether hemispheres'. Mandy might have used a body double in the rear shot of her losing her towel as she descended into the candle-lit hot tub with her blindfolded German-Guy Victim No. 2, but from all I could see from her bikini shots, she had the butt for it and didn't need a double to prove it.  Mandy's acting abilities had little to do with her impression of a psychotic 'Mommy's Girl', with the obvious erotic lesbian overtones. Her bisexual nature (allowing herself to be boinked in the hot tub after a long flirtation with German Guy No. 2, who also happened to be her mother's lover) added an additional dimension to an otherwise one-dimensional caricature of adolescent female horniness conflicted with pathological murderous impulses (always by water with the men - the ultimate fate of the Latina housekeeper was edited out in the televised version for some obscure reason).  Mandy's Uber-Nordic facial features coupled with her Uber-Voluptuous body could either be a blessing or a curse. If Mandy really wants to further her career as an actress, I'd advise her to immerse herself fully in the Romance Languages, especially Italian and Spanish - and maybe French, although I don't know if they would go for her type. But this would enable her to reconcile her Bo Derek face with her Vida Guerra body - but maybe her face is just a little too Nordic, and she has shown off too much of her extraordinary body in a cheesy movie to enable her to advance to any more fame that was enjoyed by Michelle Johnson of the 1980's whose early fame in Blame it on Rio was followed by a series of skin flicks that failed to make it off the ground.  Vambo Drule.\n",
      "Negative review(s): Seldom do I give up on a movie without seeing the entire show. This is particularly true when I have rented it on DVD. Syriana was one in which I did give up. Half way through I turned it off in bored disgust.  This movie is disjointed, boring, confusing and lackluster. The acting was dry and without credible portrayals. The general plot was good but developed in such an insipid and boring fashion that it failed to grasp my attention or interest. The multiple sub plots often failed to connect to each other and seemed more like random stories than an actual connected plot. Too bad such a serious subject and such great actors could create such a flop. I cannot imagine this movie receiving any nominations much less an award.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIF9pizTcpWm"
   },
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of raws {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "# This is a sparse matrix \n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "# This is a  \n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "# In CountVec we have the vocabulary as an attribute\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "\n",
    "# In bag_of_words we have the vector features representing each single document \n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :])\n",
    "\n",
    "# Lets get our training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values{}'.format(len(results)/len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-BsaFCwXNJQ"
   },
   "source": [
    "## Logistic Regress model using Bag of Words vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      " Number of raws 1200 (documents) -- Number of columns 17940 (vocabulary) \n",
      "\n",
      "\n",
      " Type of bag_of_words <class 'scipy.sparse.csr.csr_matrix'> \n",
      "\n",
      "\n",
      " Sparsity 0.992391397250093 \n",
      "\n",
      "\n",
      " Type of bag_of_words.toarray <class 'numpy.ndarray'> \n",
      "\n",
      "\n",
      " Type of CountVec.vocabulary <class 'dict'> \n",
      "\n",
      "A sample of CountVec.vocabulary_ [('and', 853), ('alike', 698), ('about', 304), ('above', 305), ('any', 951), ('1941', 81), ('apophis', 974), ('1928', 68), ('anderson', 856), ('along', 731), ('24', 167), ('alliances', 714), ('advanced', 505), ('antagonists', 928), ('alien', 690), ('anubis', 946), ('another', 922), ('also', 740), ('accompanies', 359), ('an', 830), ('ancients', 852), ('absolutely', 312), ('allow', 719), ('abducted', 286), ('appalling', 977), ('achievement', 384), ('afford', 548), ('against', 571), ('all', 703), ('alec', 676), ('aplomb', 967), ('annals', 905), ('airplay', 634), ('akshay', 651), ('add', 437), ('anything', 955), ('advertising', 522), ('antics', 940), ('actually', 424), ('10', 12), ('85', 251), ('amount', 813), ('alright', 739), ('anyone', 954), ('accompanied', 358), ('across', 402), ('ages', 578), ('adored', 491), ('acting', 406), ('adventures', 513), ('actors', 417), ('after', 562), ('1966', 103), ('1963', 100), ('always', 759), ('ape', 965), ('again', 570), ('anime', 900), ('2010', 157), ('30', 181), ('appearance', 985), ('al', 653), ('anna', 904), ('2nd', 179), ('america', 793), ('airborne', 623), ('affects', 540), ('apparently', 980), ('alan', 658), ('already', 738), ('anne', 906), ('amy', 829), ('american', 794), ('actual', 423), ('60', 224), ('annoyed', 917), ('1993', 132), ('2002', 147), ('alone', 730), ('appear', 984), ('adult', 498), ('admit', 476), ('almost', 729), ('alert', 677), ('amnesia', 808), ('among', 809), ('alcoholic', 671), ('apparent', 979), ('amateur', 770), ('_everything_', 271), ('actresses', 421), ('17', 44), ('actress', 420), ('acted', 404), ('agonizing', 590), ('account', 365), ('70s', 236), ('admittedly', 480), ('aired', 627), ('2006', 151), ('ann', 903), ('1920', 65), ('animals', 893), ('annoyance', 916), ('andrews', 861), ('admires', 473), ('amazing', 777), ('allowed', 720), ('angered', 875), ('ago', 586), ('ah', 598), ('accept', 339), ('act', 403), ('angry', 884), ('am', 762), ('admire', 469), ('almighty', 728), ('aniston', 902), ('adams', 429), ('afraid', 557), ('able', 293), ('actra', 418), ('although', 753), ('anthems', 931), ('70', 234), ('2am', 175), ('36', 191), ('aimed', 615), ('animated', 895), ('according', 364), ('animators', 899), ('anxious', 949), ('air', 621), ('advances', 506), ('annisten', 909), ('aliens', 695), ('1976', 114), ('23', 166), ('anywhere', 959), ('answer', 923), ('apologize', 972), ('advance', 504), ('110', 24), ('appeared', 987), ('amongst', 810), ('90', 256), ('action', 407), ('adventists', 510), ('ancestors', 845), ('anyway', 957), ('adhere', 456), ('ambiance', 782), ('1979', 117), ('20', 143), ('32', 186), ('age', 573), ('adapt', 430), ('advantage', 508), ('40', 200), ('accolades', 356), ('adults', 502), ('anti', 935), ('ancestry', 847), ('adjusting', 462), ('anew', 868), ('80', 248), ('abandoned', 279), ('50', 214), ('animation', 897), ('appears', 989), ('annoying', 918), ('1997', 136), ('aftermath', 565), ('aim', 614), ('aka', 646), ('1971', 109), ('abreast', 306), ('allison', 718), ('aircraft', 625), ('150', 39), ('anachronisms', 832), ('anyways', 958), ('appearing', 988), ('adequate', 454), ('albert', 665), ('90s', 259), ('aged', 574), ('adventurous', 515), ('abu', 321), ('ahmad', 603), ('absolute', 311), ('answering', 925), ('adaptation', 431), ('accent', 337), ('aback', 277), ('appeal', 982), ('antagonist', 927), ('amusing', 827), ('adaptations', 432), ('ads', 497), ('alive', 700), ('1978', 116), ('2003', 148), ('abilities', 291), ('almightly', 727), ('adventure', 511), ('amrapurkar', 819), ('amrapurkars', 820), ('actor', 416), ('5th', 222), ('3rd', 198), ('18', 46), ('adaption', 435), ('1940', 79), ('absurd', 318), ('applied', 998), ('anthony', 933), ('accapella', 334), ('andress', 859), ('50s', 217), ('ahead', 601), ('affected', 535), ('applaud', 993), ('agrees', 597), ('adulterous', 499), ('anthology', 932), ('ain', 618), ('3d', 197), ('35', 188), ('anniston', 910), ('anymore', 953), ('anchorman', 850), ('abuse', 325), ('added', 438), ('afternoon', 566), ('1990s', 129), ('80s', 249), ('1970s', 108), ('4th', 213), ('aircrafts', 626), ('12', 28), ('1990', 128), ('allen', 709), ('101', 16), ('15', 38), ('99', 266), ('ability', 292), ('67', 231), ('33', 187), ('alarmed', 660), ('affection', 537), ('apart', 961), ('achieved', 383), ('anticipated', 937), ('2008', 153), ('akbar', 647), ('acharya', 379), ('ali', 686), ('anil', 890), ('amps', 817), ('alpha', 737), ('adversaries', 516), ('ace', 377), ('advice', 523), ('13', 32), ('16', 41), ('accepted', 342), ('abbot', 282), ('afterlives', 564), ('angular', 888), ('amid', 799), ('affluence', 546), ('addition', 446), ('accident', 350), ('apologies', 971), ('abundance', 323), ('anytime', 956), ('alexander', 680), ('1950', 89), ('19', 57), ('1980', 118), ('amelie', 791), ('amazed', 775), ('1996', 135), ('alignment', 697), ('annual', 920), ('95', 263), ('000', 1), ('afterward', 568), ('abc', 285), ('accessible', 348), ('1969', 106), ('apartment', 962), ('1864', 52), ('aforementioned', 556), ('apocalyptic', 969), ('anybody', 952), ('14', 34), ('aboard', 296), ('1912', 63), ('animal', 891), ('absorbing', 314), ('achieve', 382), ('adjust', 461), ('alternative', 749), ('answered', 924), ('8th', 255), ('accomplish', 362), ('aluminum', 758), ('105', 18), ('alienating', 693), ('anar', 842), ('aggressive', 581), ('angrily', 883), ('agony', 591), ('akin', 648), ('abruptly', 308), ('1954', 92), ('ailing', 612), ('25', 169), ('agent', 576), ('1998', 137), ('100', 13), ('announcers', 914), ('admission', 475), ('accurate', 371), ('airlines', 630), ('accidentally', 351), ('analytical', 839), ('angora', 882), ('accused', 374), ('antoine', 942), ('angie', 878), ('alas', 661), ('1951', 91), ('adriana', 496), ('amore', 812), ('aida', 607), ('79', 246), ('2007', 152), ('absent', 310), ('agree', 593), ('andrew', 860), ('adam', 426), ('ai', 605), ('ably', 294), ('acceptance', 341), ('angels', 873), ('abound', 302), ('adversity', 518), ('aloof', 733), ('actions', 410), ('accepting', 343), ('andy', 865), ('anaconda', 833), ('alfred', 683), ('affirms', 542), ('120', 29), ('annna', 911), ('altho', 752), ('abominable', 297), ('alias', 687), ('algae', 684), ('afterthought', 567), ('ala', 654), ('acceptable', 340), ('akshaye', 652), ('aditya', 458), ('1931', 72), ('accepts', 344), ('abashed', 280), ('amann', 767), ('abrupt', 307), ('agonizes', 589), ('affair', 531), ('active', 412), ('1930s', 71), ('anxiety', 948), ('ahmed', 604), ('allan', 704), ('accomplice', 361), ('1924', 67), ('1937', 76), ('1938', 77), ('1942', 82), ('1947', 87), ('1964', 101), ('39', 196), ('adrian', 495), ('2004', 149), ('acts', 422), ('45', 206), ('anemic', 867), ('35mins', 190), ('1800', 48), ('angles', 880), ('affect', 533), ('20th', 160), ('1st', 142), ('amita', 804), ('12th', 31), ('allport', 723), ('1985', 123), ('1700', 45), ('activity', 415), ('abusive', 327), ('98', 265), ('accessibility', 347), ('30s', 184), ('40s', 204), ('11', 23), ('addictive', 443), ('adolescent', 481), ('acne', 394), ('admirably', 466), ('afterwards', 569), ('16s', 43), ('afghanistan', 551), ('admiral', 467), ('alcohol', 670), ('anchor', 848), ('advise', 524), ('absorbed', 313), ('allows', 722), ('31', 185), ('ambiguous', 785), ('21st', 164), ('102', 17), ('airphone', 631), ('ambitious', 788), ('anger', 874), ('addressed', 450), ('86', 252), ('african', 559), ('alabama', 655), ('alex', 678), ('140', 35), ('22', 165), ('05', 7), ('apathy', 964), ('ancient', 851), ('accompany', 360), ('angel', 870), ('afro', 561), ('americans', 797), ('angeles', 871), ('60th', 226), ('answers', 926), ('amazon', 779), ('agape', 572), ('address', 449), ('altman', 755), ('adler', 464), ('aladdin', 656), ('ailes', 611), ('amidst', 800), ('annoyingly', 919), ('2001', 146), ('appease', 990), ('admirers', 472), ('affairs', 532), ('ambitions', 787), ('1960s', 98), ('ally', 726), ('absence', 309), ('amazingly', 778), ('1972', 110), ('68', 232), ('abstract', 316), ('abstraction', 317), ('6th', 233), ('anorexic', 921), ('amuses', 826), ('16mm', 42), ('adding', 444), ('01', 5), ('americanized', 796), ('altogether', 756), ('09', 11), ('alicia', 689), ('adapted', 433), ('advertisements', 521), ('ancestor', 844), ('alexa', 679), ('1994', 133), ('amish', 802), ('altitude', 754), ('abydos', 329), ('activated', 411), ('addresses', 451), ('announces', 915), ('alternate', 746), ('alliance', 713), ('ahamad', 599), ('ahamd', 600), ('accumulates', 369), ('achieving', 387), ('accuracy', 370), ('alives', 701), ('1977', 115), ('1960', 97), ('appealing', 983), ('acclaimed', 354), ('adoptive', 486), ('ailments', 613), ('21', 162), ('06', 8), ('64', 229), ('addicting', 442), ('111', 25), ('360', 192), ('1956', 94), ('1980s', 119), ('apocalypse', 968), ('alledgedly', 705), ('1880', 53), ('apologized', 973), ('88', 253), ('aclear', 392), ('academy', 333), ('1900', 59), ('aldrin', 675), ('_have_', 272), ('117', 26), ('1930', 70), ('accidently', 352), ('agreed', 594), ('aerial', 526), ('amused', 824), ('1948', 88), ('admirer', 471), ('andre', 857), ('altar', 742), ('9ers', 268), ('alleys', 712), ('africa', 558), ('3000', 183), ('appearances', 986), ('1991', 130), ('acknowledge', 390), ('13th', 33), ('agonia', 587), ('aint', 620), ('aesthetic', 527), ('abuelita', 322), ('altagracia', 741), ('apartments', 963), ('5x5', 223), ('amateurish', 771), ('applicability', 996), ('alarm', 659), ('animalplanet', 892), ('albeit', 664), ('94', 262), ('amu', 822), ('antithetical', 941), ('ambiguity', 784), ('60s', 225), ('admitted', 479), ('alter', 743), ('27', 172), ('amanda', 764), ('aggressors', 582), ('2005', 150), ('alongside', 732), ('214', 163), ('amassed', 769), ('1989', 127), ('airbag', 622), ('4kids', 212), ('2000', 145), ('1929', 69), ('adds', 452), ('admired', 470), ('alki', 702), ('72', 239), ('afghans', 552), ('akki', 650), ('affections', 539), ('amazement', 776), ('1958', 95), ('andalucia', 854), ('ang', 869), ('200', 144), ('alot', 736), ('abducting', 287), ('accomplished', 363), ('abbott', 283), ('400', 201), ('1970', 107), ('anticipate', 936), ('accents', 338), ('achievements', 385), ('additional', 447), ('1974', 112), ('amityville', 806), ('1987', 125), ('81', 250), ('55th', 219), ('adventist', 509), ('26th', 171), ('amoral', 811), ('airplanes', 633), ('acquire', 397), ('1939', 78), ('applause', 995), ('71', 238), ('ambersons', 781), ('acrobatics', 401), ('alba', 663), ('akira', 649), ('airplane', 632), ('amuse', 823), ('aiming', 616), ('2fast', 177), ('2furious', 178), ('airport', 635), ('77', 245), ('747', 241), ('108', 19), ('airline', 629), ('1840', 49), ('47', 208), ('ample', 816), ('adorned', 494), ('08', 10), ('_apocalyptically', 270), ('amusingly', 828), ('abysmal', 330), ('addict', 440), ('1940s', 80), ('amaze', 774), ('achieves', 386), ('alleged', 707), ('alterior', 745), ('admittadly', 478), ('allowing', 721), ('96', 264), ('adultery', 500), ('ambassador', 780), ('1900s', 60), ('20perr', 158), ('20widow', 161), ('1943', 83), ('1920s', 66), ('activities', 414), ('angers', 876), ('1933', 74), ('aishwarya', 644), ('aishu', 643), ('analyze', 840), ('aince', 619), ('1000', 14), ('addicted', 441), ('achingly', 388), ('agents', 577), ('addled', 448), ('adores', 492), ('1907', 61), ('accusations', 373), ('applauded', 994), ('album', 668), ('76', 244), ('amateurishness', 772), ('appalled', 976), ('algerians', 685), ('aide', 608), ('adorble', 489), ('18th', 56), ('adamant', 428), ('adjoining', 460), ('adopts', 487), ('ahem', 602), ('angelina', 872), ('actioner', 408), ('10p', 20), ('albiet', 666), ('4am', 210), ('9am', 267), ('00s', 4), ('70th', 237), ('alienation', 694), ('1967', 104), ('afilm', 553), ('1999', 138), ('00', 0), ('alternates', 748), ('adopted', 485), ('accessory', 349), ('abuzz', 328), ('acquitted', 400), ('allegations', 706), ('1995', 134), ('amigos', 801), ('73', 240), ('aging', 584), ('adopt', 484), ('adversary', 517), ('adequately', 455), ('appallingly', 978), ('75', 242), ('altered', 744), ('48', 209), ('actioners', 409), ('aline', 699), ('aaja', 275), ('56', 220), ('ammo', 807), ('anand', 841), ('alcoholism', 672), ('adjective', 459), ('amusement', 825), ('1968', 105), ('accolade', 355), ('1961', 99), ('affluent', 547), ('1973', 111), ('allegedly', 708), ('anthropomorphic', 934), ('anticlimax', 939), ('90ish', 258), ('abducts', 288), ('12m', 30), ('40mins', 203), ('alfre', 682), ('adapting', 434), ('28', 173), ('agnes', 585), ('absurdist', 319), ('admirable', 465), ('abby', 284), ('addison', 445), ('alight', 696), ('appetites', 992), ('accurately', 372), ('29', 174), ('1988', 126), ('1982', 121), ('1986', 124), ('anticipation', 938), ('alters', 751), ('airwaves', 640), ('accumulated', 368), ('abused', 326), ('aids', 610), ('abandon', 278), ('alaska', 662), ('adorable', 488), ('aish', 642), ('africans', 560), ('afoot', 555), ('addendum', 439), ('alos', 735), ('ao', 960), ('8bit', 254), ('10th', 21), ('accelerated', 335), ('announce', 912), ('anecdotes', 866), ('angus', 889), ('airspace', 637), ('38k', 195), ('admiring', 474), ('abomination', 298), ('angle', 879), ('ad', 425), ('albums', 669), ('antoinette', 943), ('1946', 86), ('1862', 51), ('alienated', 692), ('apotheosis', 975), ('announced', 913), ('aisle', 645), ('absurdity', 320), ('abbey', 281), ('aesthetically', 528), ('adolph', 483), ('agrandizement', 592), ('1992', 131), ('alternatives', 750), ('007', 2), ('amounts', 814), ('afterall', 563), ('6200', 227), ('1860', 50), ('abe', 289), ('acquainted', 395), ('acquaints', 396), ('200ft', 155), ('93', 261), ('ambient', 783), ('aakrosh', 276), ('1983', 122), ('actelone', 405), ('angharad', 877), ('adjustments', 463), ('adaptor', 436), ('4eva', 211), ('affectionate', 538), ('37', 193), ('500', 215), ('analysing', 837), ('190', 58), ('allergic', 710), ('18s', 55), ('14s', 36), ('1944', 84), ('1945', 85), ('1919', 64), ('anguished', 887), ('alternately', 747), ('alibi', 688), ('1959', 96), ('angst', 885), ('150k', 40), ('ache', 380), ('afghanastan', 550), ('afl', 554), ('afganistan', 549), ('24th', 168), ('401k', 202), ('1981', 120), ('aghast', 583), ('11m', 27), ('02', 6), ('alda', 673), ('advancing', 507), ('amiss', 803), ('application', 997), ('aggravates', 579), ('airy', 641), ('agreements', 596), ('admits', 477), ('adieu', 457), ('65', 230), ('alienate', 691), ('andres', 858), ('amen', 792), ('amrish', 821), ('acknowledging', 391), ('abundant', 324), ('amateurs', 773), ('aided', 609), ('anguish', 886), ('anchored', 849), ('americanised', 795), ('agonisingly', 588), ('accommodation', 357), ('00am', 3), ('3yrs', 199), ('anu', 945), ('antes', 930), ('amanhecer', 766), ('adore', 490), ('_very_', 273), ('adama', 427), ('anders', 855), ('acid', 389), ('amitabh', 805), ('amar', 768), ('animate', 894), ('alluring', 725), ('abo', 295), ('advertised', 519), ('anally', 834), ('acclaim', 353), ('access', 345), ('aboriginal', 300), ('anglican', 881), ('airing', 628), ('acceleration', 336), ('1975', 113), ('19th', 139), ('apparition', 981), ('actreesess', 419), ('anxiously', 950), ('appeased', 991), ('andromeda', 864), ('antonellina', 944), ('anatomical', 843), ('adventuresome', 514), ('1965', 102), ('allied', 715), ('adolf', 482), ('altruistic', 757), ('actively', 413), ('apolitical', 970), ('abets', 290), ('abominations', 299), ('agenda', 575), ('abyss', 331), ('57', 221), ('adulthood', 501), ('91', 260), ('affirming', 541), ('55', 218), ('affixed', 543), ('afflict', 545), ('albinoni', 667), ('affable', 530), ('airtight', 639), ('accrued', 367), ('alley', 711), ('admiration', 468), ('amputees', 818), ('acquit', 398), ('applies', 999), ('android', 863), ('ancestral', 846), ('aces', 378), ('adventurers', 512), ('animator', 898), ('63rd', 228), ('affleck', 544), ('amair', 763), ('accounts', 366), ('07', 9), ('alain', 657), ('1950s', 90), ('antedote', 929), ('anway', 947), ('alyn', 760), ('ambling', 789), ('accusers', 375), ('707', 235), ('airbrushed', 624), ('adelaide', 453), ('animater', 896), ('alligator', 717), ('amandola', 765), ('adviser', 525), ('acheaology', 381), ('90c', 257), ('44c', 205), ('75c', 243), ('35c', 189), ('50c', 216), ('451', 207), ('affectations', 534), ('aimless', 617), ('1932', 73), ('2d', 176), ('300', 182), ('10yr', 22), ('aid', 606), ('aborigine', 301), ('aloofness', 734), ('amoured', 815), ('ambushing', 790), ('agreement', 595), ('abounds', 303), ('affecting', 536), ('acquits', 399), ('annihilates', 908), ('26', 170), ('2009', 154), ('annie', 907), ('1889', 54), ('apes', 966), ('analyse', 836), ('7th', 247), ('9th', 269), ('acme', 393), ('ambition', 786), ('allure', 724), ('aesthetics', 529), ('alderich', 674), ('advertisement', 520), ('1955', 93), ('analysis', 838), ('airstrike', 638), ('1935', 75), ('alzheimer', 761), ('aggression', 580), ('378', 194), ('adoring', 493), ('100th', 15), ('200th', 156), ('20s', 159), ('1910s', 62), ('14th', 37), ('aaa', 274), ('accessability', 346), ('advan', 503), ('animie', 901), ('absoulely', 315), ('alfonso', 681), ('androgynous', 862), ('amiable', 798), ('airs', 636), ('accustomed', 376), ('180', 47), ('1h30', 140), ('allies', 716), ('analogies', 835), ('academic', 332), ('1s', 141), ('2s', 180), ('anachronism', 831)]\n",
      "\n",
      " Type of bag_of_words.toarray() {} \n",
      "\n",
      "\n",
      " First feature vector, representing the first document \n",
      "   (0, 8867)\t1\n",
      "  (0, 8775)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 1577)\t1\n",
      "  (0, 1207)\t1\n",
      "  (0, 15731)\t1\n",
      "  (0, 1407)\t1\n",
      "  (0, 16100)\t1\n",
      "  (0, 6169)\t1\n",
      "  (0, 14838)\t1\n",
      "  (0, 7606)\t1\n",
      "  (0, 6142)\t1\n",
      "  (0, 16069)\t1\n",
      "  (0, 8781)\t1\n",
      "  (0, 9594)\t1\n",
      "  (0, 15085)\t1\n",
      "  (0, 11683)\t1\n",
      "  (0, 10593)\t2\n",
      "  (0, 17392)\t1\n",
      "  (0, 11522)\t1\n",
      "  (0, 1761)\t1\n",
      "  (0, 9987)\t1\n",
      "  (0, 12164)\t1\n",
      "  (0, 2397)\t1\n",
      "  (0, 8127)\t1\n",
      "  :\t:\n",
      "  (0, 8648)\t1\n",
      "  (0, 15292)\t3\n",
      "  (0, 6658)\t2\n",
      "  (0, 14327)\t1\n",
      "  (0, 11526)\t1\n",
      "  (0, 5667)\t1\n",
      "  (0, 10591)\t1\n",
      "  (0, 8575)\t5\n",
      "  (0, 7423)\t2\n",
      "  (0, 6631)\t3\n",
      "  (0, 6800)\t2\n",
      "  (0, 2352)\t1\n",
      "  (0, 3074)\t1\n",
      "  (0, 9818)\t1\n",
      "  (0, 6666)\t1\n",
      "  (0, 698)\t1\n",
      "  (0, 9552)\t1\n",
      "  (0, 7643)\t2\n",
      "  (0, 853)\t3\n",
      "  (0, 8644)\t2\n",
      "  (0, 8703)\t2\n",
      "  (0, 1127)\t4\n",
      "  (0, 15155)\t1\n",
      "  (0, 13581)\t5\n",
      "  (0, 13667)\t2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8566666666666667\n",
      "Percentage of correct predicted values0.8566666666666667\n"
     ]
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of raws {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "# This is a sparse matrix \n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "# This is a  \n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "# In CountVec we have the vocabulary as an attribute\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "\n",
    "# In bag_of_words we have the vector features representing each single document \n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :])\n",
    "\n",
    "# Lets get our training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values{}'.format(len(results)/len(y_test)))\n",
    "\n",
    "\n",
    "# lr_CV = Pipeline([('vect', CountVec), ('clf', LogisticRegression(random_state=0))])\n",
    "\n",
    "\n",
    "\n",
    "# lr_CV.fit(X_train, y_train)\n",
    "# print('Train accuracy {}'.format(lr_CV.score(X_train, y_train)))\n",
    "# print('Test accuracy {}'.format(lr_CV.score(X_test, y_test)))\n",
    "\n",
    "# # # Trying with cross_val_score\n",
    "# lr = LogisticRegression()\n",
    "# k_folds = 10\n",
    "# X_train_CV = CountVec.fit_transform(X_train)\n",
    "# type(X_train_CV)\n",
    "# print('Train accuracy list {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds))) \n",
    "# print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "-S-I4Fc2XNJS",
    "outputId": "7158b946-8734-47df-d4f4-9318b9d2a0c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 1.0\n",
      "Test accuracy 0.83\n",
      "Train accuracy list [0.87417219 0.74172185 0.86092715 0.76821192 0.83333333 0.82\n",
      " 0.82550336 0.80536913 0.82550336 0.81208054] \n",
      "Train accuracy mean 0.8166822821755041 \n"
     ]
    }
   ],
   "source": [
    "<<<<<<< LOCAL CELL DELETED >>>>>>>\n",
    "CountVec = CountVectorizer()\n",
    "lr_CV = Pipeline([('vect', CountVec), ('clf', LogisticRegression(random_state=0))])\n",
    "lr_CV.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_CV.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_CV.score(X_test, y_test)))\n",
    "\n",
    "# # Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_CV = CountVec.fit_transform(X_train)\n",
    "type(X_train_CV)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeUltp6gXNJd"
   },
   "source": [
    "## Logistic Regress model using TfidfVectorizer vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "s16mkhktXNJf",
    "outputId": "6fce5e0d-1d45-432e-e7cb-f1fd5972bc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9673333333333334\n",
      "Test accuracy 0.83\n",
      "Train accuracy list [0.81456954 0.79470199 0.80794702 0.7615894  0.82       0.83333333\n",
      " 0.85234899 0.81208054 0.82550336 0.79865772] \n",
      "Train accuracy mean 0.8120731884380048 \n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])\n",
    "lr_tfidf.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_tfidf.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_tfidf.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2lMqoGKXNJl"
   },
   "source": [
    "## Logistic Regress model using TfidfVectorizer and different values for C hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "zZZOXbA4XNJm",
    "outputId": "518f52ab-2ccd-4bca-9105-5fdcf5c797ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_value 1.0 Test Score 0.83 Train_score 0.9673333333333334\n",
      "C_value 1.1 Test Score 0.828 Train_score 0.9693333333333334\n",
      "C_value 1.2000000000000002 Test Score 0.832 Train_score 0.974\n",
      "C_value 1.3000000000000003 Test Score 0.834 Train_score 0.9766666666666667\n",
      "C_value 1.4000000000000004 Test Score 0.832 Train_score 0.9786666666666667\n",
      "C_value 1.5000000000000004 Test Score 0.83 Train_score 0.9786666666666667\n",
      "C_value 1.6000000000000005 Test Score 0.828 Train_score 0.9793333333333333\n",
      "C_value 1.7000000000000006 Test Score 0.828 Train_score 0.9813333333333333\n",
      "C_value 1.8000000000000007 Test Score 0.83 Train_score 0.9833333333333333\n",
      "C_value 1.9000000000000008 Test Score 0.83 Train_score 0.984\n"
     ]
    }
   ],
   "source": [
    "C_values = np.arange(1,2,0.1)\n",
    "results = []\n",
    "\n",
    "for value in C_values:   \n",
    "    lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0, C=value))])\n",
    "    lr_tfidf.fit(X_train, y_train)\n",
    "    train_score = lr_tfidf.score(X_train, y_train)\n",
    "    score = lr_tfidf.score(X_test, y_test)\n",
    "    print('C_value {} Test Score {} Train_score {}'.format(value, score, train_score))\n",
    "    results.append(score)\n",
    "\n",
    "time_end_of_notebook = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "6siRLHQU79F7",
    "outputId": "dbc21ee2-1066-4b68-c8a0-ad47ca57f4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1000\n",
      "Full notebook execution duration: 124.60530209541321 seconds\n",
      "Full notebook execution duration: 2.0767550349235533 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Vectorisation techniques</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>0.984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models Vectorisation techniques    Score\n",
       "0  Logistic Regression             Bag of Words     0.83\n",
       "1  Logistic Regression                 Word2Vec  Pending\n",
       "2  Logistic Regression                    TFIDF    0.984"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_models_vectorization = pd.DataFrame(\n",
    "     {'Models':                   [\"Logistic Regression\", \"Logistic Regression\", \"Logistic Regression\"], \n",
    "      'Vectorisation techniques': [\"Bag of Words\",        \"Word2Vec\", \"TFIDF\"], \n",
    "      'Score':                    [score,                 \"Pending\", lr_tfidf.score(X_train, y_train) ]},\n",
    "    columns=['Models','Vectorisation techniques','Score']\n",
    ")\n",
    "print(\"Sample size:\", SAMPLE_SIZE)\n",
    "\n",
    "duration = time_end_of_notebook - time_beginning_of_notebook\n",
    "\n",
    "print(\"Full notebook execution duration:\", duration, \"seconds\")\n",
    "print(\"Full notebook execution duration:\", duration / 60, \"minutes\")\n",
    "\n",
    "table_models_vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-28yfXCkgTe"
   },
   "source": [
    "**The below two code blocks replaces the original/inital BoW implementation using Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      " Number of raws 2000 (documents) -- Number of columns 25782 (vocabulary) \n",
      "\n",
      "\n",
      " Type of bag_of_words <class 'scipy.sparse.csr.csr_matrix'> \n",
      "\n",
      "\n",
      " Sparsity 0.9945600224963153 \n",
      "\n",
      "\n",
      " Type of bag_of_words.toarray <class 'numpy.ndarray'> \n",
      "\n",
      "\n",
      " Type of CountVec.vocabulary <class 'dict'> \n",
      "\n",
      "A sample of CountVec.vocabulary_ [('alexander', 890), ('1940', 106), ('1950', 117), ('also', 977), ('2002', 183), ('again', 762), ('after', 750), ('ago', 792), ('about', 405), ('10', 11), ('ah', 805), ('accept', 449), ('act', 532), ('almost', 960), ('actually', 558), ('admire', 620), ('all', 922), ('acting', 534), ('12', 30), ('75', 317), ('action', 536), ('actors', 550), ('actual', 555), ('adds', 596), ('aesthetic', 707), ('add', 577), ('114', 29), ('1986', 157), ('adult', 664), ('alive', 920), ('although', 993), ('according', 478), ('1914', 86), ('30', 233), ('along', 964), ('adults', 669), ('added', 579), ('across', 531), ('afternoon', 754), ('adept', 600), ('accomplished', 473), ('20', 177), ('15', 41), ('ahead', 806), ('affair', 713), ('agent', 772), ('007', 2), ('70', 306), ('additional', 588), ('allow', 946), ('air', 830), ('alone', 963), ('aline', 916), ('actresses', 553), ('almighty', 958), ('adams', 569), ('afraid', 743), ('50', 276), ('acid', 513), ('60', 295), ('adam', 566), ('1988', 159), ('50s', 282), ('aforementioned', 741), ('adaptation', 572), ('actions', 539), ('allowed', 947), ('actress', 552), ('albeit', 870), ('accompanied', 468), ('allows', 949), ('against', 763), ('acts', 554), ('acted', 533), ('agatha', 765), ('age', 766), ('allen', 929), ('ageing', 768), ('actor', 549), ('abhorrent', 380), ('agents', 773), ('accomplish', 472), ('40', 257), ('18', 54), ('22', 204), ('absolutely', 417), ('advance', 670), ('1938', 103), ('1978', 147), ('1982', 153), ('1983', 154), ('1987', 158), ('1930', 95), ('advantage', 675), ('able', 385), ('abundant', 430), ('accepted', 452), ('already', 973), ('35', 243), ('3rd', 255), ('additions', 590), ('afford', 733), ('abuse', 431), ('abused', 432), ('advocate', 702), ('1936', 102), ('altering', 984), ('alternate', 986), ('addition', 587), ('aid', 811), ('alucarda', 998), ('alien', 906), ('ability', 383), ('abandoned', 364), ('1968', 135), ('advice', 695), ('alice', 904), ('1st', 176), ('27', 220), ('2009', 190), ('agree', 799), ('akria', 848), ('140', 36), ('33', 241), ('altman', 994), ('2004', 185), ('1970', 138), ('2000', 179), ('alas', 864), ('abilities', 382), ('aj', 843), ('101', 18), ('80', 321), ('64', 299), ('adjacent', 608), ('adapted', 574), ('2005', 186), ('29th', 226), ('2008', 189), ('admit', 626), ('accent', 443), ('16', 46), ('abound', 402), ('achievement', 504), ('1906', 81), ('1100', 26), ('admiration', 619), ('above', 406), ('alex', 889), ('ailing', 818), ('84', 326), ('06', 8), ('1950s', 118), ('acquainted', 525), ('aboriginal', 394), ('accused', 495), ('1977', 146), ('aborigines', 397), ('aborigine', 396), ('acknowledge', 515), ('25', 216), ('adorned', 650), ('academy', 440), ('altogether', 996), ('300', 234), ('2006', 187), ('aggravate', 777), ('aim', 819), ('alternatively', 990), ('1974', 143), ('2007', 188), ('adorable', 644), ('alluring', 953), ('acoustic', 521), ('afterwards', 759), ('80s', 323), ('acceptable', 450), ('abounding', 403), ('absence', 413), ('abnormal', 388), ('11', 24), ('2001', 182), ('70s', 310), ('alleyway', 935), ('000', 1), ('advised', 697), ('activities', 547), ('addressed', 593), ('affect', 715), ('allowing', 948), ('admitted', 630), ('alan', 859), ('136', 33), ('1971', 140), ('accounts', 485), ('78', 318), ('achieved', 503), ('1h53', 174), ('alright', 975), ('aimee', 822), ('adventures', 682), ('africa', 745), ('90', 332), ('african', 746), ('1960', 127), ('accomplice', 471), ('1972', 141), ('14', 35), ('6th', 305), ('85', 327), ('al', 853), ('afv', 760), ('1959', 126), ('addressing', 595), ('acceptance', 451), ('agreeing', 801), ('absorbing', 419), ('absolute', 416), ('195', 116), ('alliance', 936), ('achieves', 506), ('100', 12), ('absurd', 423), ('aimed', 821), ('alicia', 905), ('44', 263), ('1920', 88), ('alliances', 937), ('adjusted', 610), ('3am', 251), ('18th', 77), ('aikens', 817), ('alison', 917), ('20s', 198), ('1910s', 84), ('30s', 236), ('aired', 832), ('adventure', 679), ('accidentally', 461), ('accurate', 491), ('accuracy', 490), ('adrian', 657), ('aboard', 389), ('accurately', 492), ('adorns', 652), ('accounted', 484), ('altered', 983), ('aging', 785), ('aime', 820), ('aged', 767), ('3000', 235), ('20th', 199), ('1999', 171), ('abroad', 409), ('ale', 883), ('accident', 459), ('adding', 585), ('abc', 370), ('alike', 915), ('adolescent', 635), ('alfonso', 895), ('99', 343), ('ad', 562), ('accessible', 458), ('00', 0), ('400', 258), ('abomination', 392), ('1979', 148), ('32', 239), ('adapt', 571), ('60s', 297), ('alias', 901), ('abrams', 408), ('agreement', 802), ('_blair', 348), ('aince', 829), ('24', 213), ('abstractions', 422), ('addictive', 583), ('adriana', 658), ('albacore', 868), ('abe', 374), ('allmighty', 941), ('1896', 76), ('1954', 121), ('adequate', 601), ('adaptations', 573), ('45', 264), ('26', 219), ('achieve', 502), ('akshaye', 851), ('2nd', 230), ('absurdity', 425), ('admirably', 616), ('alcohol', 879), ('access', 455), ('aching', 510), ('ages', 774), ('advanced', 671), ('abduction', 373), ('16mm', 50), ('agis', 786), ('204', 192), ('1960s', 128), ('1hr', 175), ('45min', 266), ('50min', 281), ('4cylinder', 270), ('140hp', 37), ('1993', 165), ('accord', 477), ('40mph', 260), ('abstract', 421), ('affirmed', 728), ('1994', 166), ('1997', 169), ('1967', 134), ('1966', 133), ('ain', 828), ('aankh', 357), ('1973', 142), ('alita', 919), ('56', 289), ('31', 237), ('ajay', 844), ('71', 311), ('1933', 99), ('airs', 840), ('airing', 834), ('agi', 783), ('alleys', 934), ('alley', 933), ('addicted', 581), ('adrenalin', 654), ('advances', 673), ('1946', 113), ('aborted', 398), ('alter', 981), ('1920s', 89), ('abyss', 437), ('1981', 152), ('alain', 857), ('alexandre', 892), ('68', 302), ('215', 202), ('1996', 168), ('13', 31), ('achievements', 505), ('agendas', 771), ('900', 333), ('ahu', 809), ('49', 269), ('2040', 193), ('adorably', 645), ('absurdist', 424), ('1964', 132), ('1953', 120), ('aka', 845), ('abominable', 391), ('alarm', 860), ('advise', 696), ('1980', 150), ('adjani', 609), ('1992', 164), ('4ever', 272), ('aloof', 967), ('8mm', 331), ('allan', 923), ('1928', 93), ('1839', 62), ('89', 330), ('allies', 938), ('alienation', 909), ('102', 19), ('acknowledged', 516), ('28', 223), ('acclaimed', 464), ('79', 319), ('adversary', 686), ('9as', 344), ('21', 201), ('1991', 163), ('affairs', 714), ('adjustment', 611), ('abortions', 400), ('accented', 444), ('allegedly', 927), ('1000', 13), ('aluminium', 999), ('1941', 108), ('2044', 194), ('agrees', 803), ('15th', 45), ('affluent', 732), ('admits', 628), ('adieu', 606), ('0f', 10), ('aliens', 911), ('13th', 34), ('90s', 335), ('aeons', 704), ('1957', 124), ('adela', 597), ('61', 298), ('1930s', 96), ('alamo', 858), ('absent', 414), ('1800', 56), ('active', 542), ('ace', 498), ('adreon', 656), ('allover', 945), ('53', 285), ('altaire', 979), ('1956', 123), ('2200', 206), ('accompany', 469), ('affectionate', 720), ('affects', 724), ('abandoning', 365), ('65', 300), ('165', 49), ('1890s', 73), ('2000s', 181), ('2080', 197), ('2070', 196), ('acp', 522), ('affected', 718), ('ahh', 808), ('affectionnates', 722), ('airplane', 837), ('alot', 968), ('advancement', 672), ('accidents', 462), ('abolish', 390), ('50th', 283), ('abuelita', 428), ('altagracia', 978), ('alpha', 970), ('aftermath', 753), ('5x5', 294), ('aloud', 969), ('agnostic', 789), ('address', 592), ('afterlife', 752), ('admittedly', 631), ('aime', 827), ('38', 249), ('albert', 872), ('abusive', 434), ('1980s', 151), ('36', 247), ('112', 28), ('alibis', 903), ('acquit', 528), ('05', 7), ('19th', 172), ('accomplishment', 476), ('88', 329), ('albot', 875), ('14th', 39), ('83', 325), ('abraham', 407), ('adopted', 641), ('aesthetically', 708), ('82', 324), ('admired', 621), ('aggressive', 780), ('alert', 888), ('advancing', 674), ('accumulation', 489), ('alba', 867), ('aims', 826), ('600', 296), ('10th', 23), ('alaskan', 865), ('albas', 869), ('accepts', 454), ('ahem', 807), ('advertisement', 691), ('4kids', 274), ('adviser', 698), ('agonizing', 796), ('adversity', 688), ('alloted', 943), ('agony', 798), ('alligator', 939), ('21st', 203), ('accuse', 494), ('agenda', 770), ('akshay', 849), ('afloat', 740), ('adverse', 687), ('40s', 261), ('_not_', 352), ('______', 346), ('almodovar', 959), ('afterthought', 756), ('abysmal', 435), ('actioner', 537), ('admirer', 622), ('alters', 991), ('adjustments', 612), ('allayli', 924), ('1940s', 107), ('200', 178), ('95', 339), ('acclaim', 463), ('66', 301), ('accolade', 465), ('adore', 647), ('accentuated', 447), ('1927', 92), ('admission', 625), ('70ies', 308), ('1910', 83), ('advertises', 692), ('alternative', 989), ('abortive', 401), ('actively', 543), ('2003', 184), ('achingly', 511), ('10s', 22), ('allegory', 928), ('acquired', 527), ('admiring', 624), ('affleck', 729), ('1948', 114), ('activist', 545), ('activism', 544), ('alienating', 908), ('aimless', 824), ('addiction', 582), ('adrenaline', 655), ('adrienne', 660), ('alien', 912), ('altar', 980), ('180', 55), ('1h30', 173), ('aberration', 376), ('adverts', 694), ('1902', 80), ('agreed', 800), ('alfredo', 898), ('akin', 846), ('aiming', 823), ('2fast', 228), ('2furious', 229), ('ada', 563), ('1984', 155), ('1989', 160), ('accents', 445), ('achilles', 509), ('alastair', 866), ('1951', 119), ('1939', 104), ('1942', 109), ('agito', 788), ('110', 25), ('adoption', 642), ('abortion', 399), ('1995', 167), ('accepting', 453), ('164', 48), ('94', 338), ('17', 51), ('adultery', 667), ('addison', 586), ('abby', 369), ('abbey', 367), ('admittance', 629), ('accompanying', 470), ('afoul', 742), ('alarming', 862), ('aggravating', 778), ('adamson', 570), ('1934', 100), ('4th', 275), ('affection', 719), ('albums', 877), ('actullly', 559), ('alrite', 976), ('230mph', 210), ('activity', 548), ('250', 217), ('alejandro', 886), ('afghan', 736), ('allende', 930), ('abundance', 429), ('accomplishes', 474), ('acorn', 519), ('aardvarks', 359), ('31st', 238), ('administration', 614), ('accommodate', 467), ('90mins', 334), ('500', 277), ('accidental', 460), ('9th', 345), ('adagio', 565), ('_the', 353), ('_mystery', 351), ('alongside', 966), ('alok', 962), ('account', 481), ('adolf', 637), ('alluding', 951), ('agnosticism', 790), ('academic', 438), ('6k', 304), ('acurately', 561), ('86', 328), ('abel', 375), ('airline', 835), ('allergic', 931), ('1976', 145), ('03', 6), ('aiello', 816), ('01', 4), ('addresses', 594), ('agonize', 794), ('1975', 144), ('1990', 161), ('abandons', 366), ('abruptly', 411), ('1970s', 139), ('abandon', 363), ('1935', 101), ('1998', 170), ('albniz', 878), ('1860', 70), ('1909', 82), ('albeniz', 871), ('alfre', 896), ('alistair', 918), ('409', 259), ('1943', 110), ('accumulates', 488), ('alleged', 926), ('02', 5), ('afterward', 758), ('aircraft', 831), ('afar', 710), ('747', 315), ('admirers', 623), ('airwaves', 842), ('alfred', 897), ('1949', 115), ('170', 52), ('abduct', 371), ('abrupt', 410), ('alahani', 856), ('alohalani', 961), ('addicts', 584), ('7mm', 320), ('47', 267), ('93', 337), ('airport', 839), ('alight', 913), ('alcoholic', 880), ('aakash', 356), ('ado', 633), ('adamant', 567), ('alexandra', 891), ('aimlessly', 825), ('activate', 540), ('advertised', 690), ('abm', 387), ('academics', 439), ('3d', 252), ('2d', 227), ('algae', 899), ('abbott', 368), ('adolphe', 638), ('adulthood', 668), ('affectionately', 721), ('abducted', 372), ('23rd', 212), ('ally', 955), ('admirable', 615), ('3who', 256), ('abominations', 393), ('5000', 278), ('affirmations', 727), ('alexei', 893), ('1000lb', 14), ('advent', 677), ('aerial', 705), ('affections', 723), ('abject', 384), ('adaption', 576), ('adored', 648), ('20x', 200), ('adroitly', 662), ('acre', 529), ('98', 342), ('1969', 136), ('afghanistan', 738), ('akshaya', 850), ('allison', 940), ('allusions', 954), ('39', 250), ('advertising', 693), ('aboriginals', 395), ('admitting', 632), ('1990s', 162), ('2022', 191), ('alienate', 907), ('350', 244), ('58', 291), ('500ad', 279), ('accessibility', 457), ('73', 313), ('accountants', 483), ('advantages', 676), ('aauugghh', 361), ('74th', 316), ('accessability', 456), ('aggression', 779), ('alberto', 873), ('454', 265), ('acquaintances', 524), ('3lbs', 253), ('album', 876), ('aaron', 360), ('altho', 992), ('adi', 605), ('alternately', 987), ('10p', 21), ('albiet', 874), ('96', 340), ('addam', 578), ('19', 78), ('affectations', 717), ('ag', 761), ('acosta', 520), ('abusing', 433), ('acetylene', 500), ('affable', 712), ('aggelopoulos', 775), ('alekos', 887), ('achilleas', 508), ('afro', 748), ('afghani', 737), ('africans', 747), ('adelin', 599), ('1918', 87), ('abetted', 378), ('airplanes', 838), ('alternates', 988), ('airtight', 841), ('afresh', 744), ('allotted', 944), ('alcoholics', 881), ('acme', 518), ('agency', 769), ('1926', 91), ('akst', 852), ('1963', 131), ('accustomed', 497), ('agin', 784), ('absurdly', 426), ('alludes', 950), ('admiral', 617), ('alcoholism', 882), ('adopts', 643), ('adventists', 678), ('aggrandizement', 776), ('aids', 815), ('adapting', 575), ('2046', 195), ('aggressiveness', 781), ('500lbs', 280), ('1958', 125), ('alienator', 910), ('accumulated', 487), ('5mins', 292), ('1955', 122), ('70mm', 309), ('aficionado', 739), ('absorbs', 420), ('6am', 303), ('1300', 32), ('adroit', 661), ('5th', 293), ('acquire', 526), ('accusations', 493), ('adolescence', 634), ('advises', 700), ('adolescents', 636), ('achterbusch', 512), ('1962', 130), ('1900', 79), ('admirals', 618), ('actio', 535), ('700', 307), ('afflictions', 731), ('afforded', 735), ('42nd', 262), ('34', 242), ('altruistic', 997), ('activating', 541), ('alarms', 863), ('afer', 711), ('20000', 180), ('177', 53), ('72', 312), ('1892', 74), ('alexis', 894), ('23', 209), ('allegations', 925), ('actreesess', 551), ('affiliation', 726), ('aegean', 703), ('airliner', 836), ('adulterer', 665), ('adhere', 603), ('54', 286), ('addict', 580), ('150', 42), ('237', 211), ('agonising', 793), ('aditya', 607), ('addled', 591), ('alleviate', 932), ('____________________________________', 347), ('afternoons', 755), ('57', 290), ('abigail', 381), ('1985', 156), ('aggrivating', 782), ('183', 58), ('197', 137), ('220', 205), ('100x', 17), ('alibi', 902), ('adventurous', 685), ('altmanesque', 995), ('alka', 921), ('absentminded', 415), ('alarmed', 861), ('ala', 854), ('aag', 355), ('aided', 813), ('ache', 501), ('97', 341), ('1500', 43), ('adoring', 649), ('360', 248), ('adopt', 640), ('additionally', 589), ('accountancy', 482), ('55', 287), ('adequately', 602), ('aires', 833), ('alterio', 985), ('24th', 215), ('akira', 847), ('29', 225), ('affectation', 716), ('a10', 354), ('150m', 44), ('1847', 64), ('align', 914), ('adventuresome', 683), ('abhorrence', 379), ('afflicted', 730), ('alabama', 855), ('absconded', 412), ('35yr', 246), ('adulating', 663), ('activists', 546), ('afterthoughts', 757), ('_get_', 350), ('52', 284), ('1945', 112), ('146', 38), ('911', 336), ('adhering', 604), ('abysmally', 436), ('aide', 812), ('advisers', 699), ('32lb', 240), ('alecky', 885), ('alma', 957), ('admitedly', 627), ('alongs', 965), ('abu', 427), ('aesthetics', 709), ('adr', 653), ('1912', 85), ('800', 322), ('1859', 69), ('1880', 71), ('555', 288), ('28th', 224), ('1895', 75), ('aerobics', 706), ('accuses', 496), ('74', 314), ('actuall', 557), ('adrien', 659), ('algy', 900), ('ab', 362), ('aapkey', 358), ('adolphs', 639), ('abounds', 404), ('agonizes', 795), ('actioners', 538), ('advert', 689), ('100m', 16), ('accolades', 466), ('alphaville', 972), ('_extremeley_', 349), ('allying', 956), ('1929', 94), ('alteration', 982), ('accorsi', 480), ('affiliate', 725), ('1836', 60), ('1854', 67), ('1830', 59), ('1855', 68), ('1850s', 65), ('1838', 61), ('1846', 63), ('48', 268), ('1853', 66), ('161', 47), ('acapella', 441), ('accentuate', 446), ('aiding', 814), ('affordable', 734), ('acquaintance', 523), ('accomplishing', 475), ('adage', 564), ('272', 221), ('273', 222), ('ai', 810), ('14yr', 40), ('25s', 218), ('1100ad', 27), ('advisor', 701), ('00pm', 3), ('08', 9), ('1931', 97), ('acerbic', 499), ('1961', 129), ('adele', 598), ('achieving', 507), ('3pm', 254), ('acronym', 530), ('2210', 207), ('afterbirth', 751), ('1944', 111), ('afrovideo', 749), ('agust', 804), ('acidic', 514), ('4eva', 271), ('223', 208), ('alredy', 974), ('2oo4', 231), ('2oo5', 232), ('197o', 149), ('188o', 72), ('1924', 90), ('1932', 98), ('193o', 105), ('adorning', 651), ('agitator', 787), ('aberystwyth', 377), ('acknowledges', 517), ('1800s', 57), ('acumen', 560), ('adoration', 646), ('absorbed', 418), ('allocation', 942), ('100b', 15), ('35mm', 245), ('accrutements', 486), ('105', 20), ('ably', 386), ('agnus', 791), ('alec', 884), ('accordingly', 479), ('adulterous', 666), ('allurement', 952), ('accelerating', 442), ('agamemnon', 764), ('accentuates', 448), ('actuality', 556), ('agonizingly', 797), ('4f', 273), ('241', 214), ('alphabet', 971), ('adler', 613), ('adamantly', 568), ('adventurer', 680), ('adventurers', 681), ('adventuring', 684)]\n",
      "\n",
      " Type of bag_of_words.toarray() {} \n",
      "\n",
      "\n",
      " First feature vector, representing the first document \n",
      " \n"
     ]
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of raws {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "# This is a sparse matrix \n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "# This is a  \n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "# In CountVec we have the vocabulary as an attribute\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "\n",
    "# In bag_of_words we have the vector features representing each single document \n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6A2pKzOmxzT"
   },
   "outputs": [],
   "source": [
    "# Lets get our training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "pqbPNSGUg4Rj",
    "outputId": "b5976680-a41d-4736-99b7-e5e17ce473f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using score function: 0.826\n",
      "Percentage of correct predicted values: 0.826\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Using score function: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values: {}'.format(len(results)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0YIRBaroMeM"
   },
   "source": [
    "### RandomForestClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nMUUsMLkkyLO",
    "outputId": "8b2096de-ded8-4e02-b75a-046b1861f529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using score function: 0.836\n",
      "Percentage of correct predicted values: 0.836\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Using score function: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values: {}'.format(len(results)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "### RandomForestClassifer with TFIDF (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "HNluLDB4nayq",
    "outputId": "2ee1d453-891e-45d9-ad20-2d8bd81c3853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 1.0\n",
      "Test accuracy 0.824\n",
      "Train accuracy list [0.78807947 0.82       0.70666667 0.84       0.82       0.80666667\n",
      " 0.86       0.84666667 0.83333333 0.82550336] \n",
      "Train accuracy mean 0.8146916159236707 \n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', RandomForestClassifier(n_estimators=1000))])\n",
    "lr_tfidf.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_tfidf.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_tfidf.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjDcOMEUoToE"
   },
   "source": [
    "### LSTM with Keras (Sequential model)\n",
    "\n",
    "\n",
    "Please note that the below code is executed on GPU instances on Colab, this wont work on your local machine, use the flag to enable/disable running in CPU or GPU mode, set `run_in_GPU_mode_on_colab=false` in order to be able to run in CPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "fVN5C_34oSgO",
    "outputId": "03f65fd1-c9fb-47dc-8025-afa10d4f68d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/2\n",
      "1350/1350 [==============================] - 62s 46ms/step - loss: 0.6947 - acc: 0.4822 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 2/2\n",
      "1350/1350 [==============================] - 59s 43ms/step - loss: 0.6934 - acc: 0.4919 - val_loss: 0.6928 - val_acc: 0.5333\n",
      "500/500 [==============================] - 5s 10ms/step\n",
      "Test score: 0.6931333312988281\n",
      "Test accuracy: 0.4959999995231628\n",
      "Duration on the CPU: 128.34940028190613\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def lstm_keras():\n",
    "  from keras.models import Sequential\n",
    "  from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "  from keras.preprocessing.text import Tokenizer\n",
    "  from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "  vocab_size = 1000\n",
    "  tokenize = Tokenizer(num_words=vocab_size)\n",
    "  tokenize.fit_on_texts(X_train)\n",
    "\n",
    "  encoded_X_train = tokenize.texts_to_matrix(X_train)\n",
    "  encoded_X_test = tokenize.texts_to_matrix(X_test)\n",
    "\n",
    "  encoder = LabelBinarizer()\n",
    "  encoder.fit(y_train)\n",
    "  encoded_y_train = encoder.transform(y_train)\n",
    "  encoded_y_test = encoder.transform(y_test)\n",
    "\n",
    "  max_features = 1000\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "  model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  # try using a GRU instead, for fun\n",
    "  model.add(Dense(1))\n",
    "  model.add(Activation('sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "  batch_size=64\n",
    "  epochs=10\n",
    "  history = model.fit(encoded_X_train, encoded_y_train, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  verbose=1, \n",
    "                  validation_split=0.1)\n",
    "\n",
    "  score = model.evaluate(encoded_X_test, encoded_y_test, \n",
    "                         batch_size=batch_size, verbose=1)\n",
    "  print('Test score:', score[0])\n",
    "  print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "run_in_GPU_mode_on_colab=False\n",
    "\n",
    "if run_in_GPU_mode_on_colab:  \n",
    "  config = tf.ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "\n",
    "  with tf.device('/gpu:0'):\n",
    "    session_gpu = tf.Session(config=config)\n",
    "    session_gpu.run(tf.global_variables_initializer())\n",
    "    session_gpu.run(tf.tables_initializer())\n",
    "    start = time.time()\n",
    "    session_gpu.run(lstm_keras())\n",
    "    end = time.time()\n",
    "    gpu_time = end - start\n",
    "    print('Duration on the GPU: {} seconds'.format(gpu_time))\n",
    "else:\n",
    "  start = time.time()\n",
    "  lstm_keras()\n",
    "  end = time.time()\n",
    "  cpu_time = end - start\n",
    "  print('Duration on the CPU: {} seconds'.format(cpu_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvChJ9kdot4q"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment analysis of movies (IMDB).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
