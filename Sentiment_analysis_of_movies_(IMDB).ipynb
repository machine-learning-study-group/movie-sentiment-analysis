{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0XMzfB5_EtE"
   },
   "source": [
    "### Sentiment analysis of movie (IMDB) reviews using dataset provided by the ACL 2011 paper, see http://ai.stanford.edu/~amaas/data/sentiment/.\n",
    "\n",
    "#### Dataset can be downloaded separately from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz, but wont be necessary as the download process has been embedded in the notebook and source file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "CML_IG6z-iwM",
    "outputId": "d9301f36-c1cf-4b3f-e639-a731880ed036"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install --upgrade gensim\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import nltk\n",
    "\n",
    "import wget\n",
    "import tarfile\n",
    "import glob\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FJiWamI00hBp",
    "outputId": "e3c105d5-037f-4521-b8e1-a83cb7a5edeb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [####################################################]           80M / 80M"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('aclImdb_v1.tar.gz'):\n",
    "    wget.download('http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz')   \n",
    "\n",
    "if not os.path.isdir('aclImdb'):  \n",
    "    tar = tarfile.open(\"aclImdb_v1.tar.gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U5Tnmoh-Dpfk"
   },
   "outputs": [],
   "source": [
    "time_beginning_of_notebook = time.time()\n",
    "SAMPLE_SIZE=1000\n",
    "positive_sample_file_list = glob.glob(os.path.join('aclImdb/train/pos', \"*.txt\"))\n",
    "positive_sample_file_list = positive_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "negative_sample_file_list = glob.glob(os.path.join('aclImdb/train/neg', \"*.txt\"))\n",
    "negative_sample_file_list = negative_sample_file_list[:SAMPLE_SIZE]\n",
    "\n",
    "import re\n",
    "\n",
    "# load doc into memory\n",
    "# regex to clean markup elements \n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r', encoding='utf8')\n",
    "    # read all text\n",
    "    text = re.sub('<[^>]*>', ' ', file.read())\n",
    "    #text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMYBkcdIB9uc"
   },
   "source": [
    "# Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cz5eJi7AGSqR"
   },
   "outputs": [],
   "source": [
    "positive_strings = [load_doc(x) for x in positive_sample_file_list]\n",
    "#print('\\n Positive reviews \\n ',positive_strings[:5])\n",
    "\n",
    "negative_strings = [load_doc(x) for x in negative_sample_file_list]\n",
    "#print('\\n Negative reviews \\n ', negative_strings[:5])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C0WiHTr7I4CN"
   },
   "outputs": [],
   "source": [
    "positive_tokenized = [word_tokenize(s) for s in positive_strings]\n",
    "#print('\\n Positive tokenized 1 \\n {} \\n\\n Positive tokenized 2 \\n {}'. format(positive_tokenized[1], positive_tokenized[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YDP-eqAGIq5R"
   },
   "outputs": [],
   "source": [
    "negative_tokenized = [word_tokenize(s) for s in negative_strings]\n",
    "#print('\\n Negative tokenized 1 \\n {} \\n\\n  Negative tokenized 2 \\n {}'. format(negative_tokenized[1], negative_tokenized[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "6bgN1KJRMPpq",
    "outputId": "aa69f5b0-246f-4805-bc36-9944c1f36b3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 269386\n",
      "Non alphanumeric characters found in universe vocabulary {':', '?', ')', '!', ']', '(', ';', '=', '[', '-', '}', \"'\"}\n",
      "Word count across all reviews (after stripping tokens): 233406\n"
     ]
    }
   ],
   "source": [
    "# load doc into memory\n",
    "with open('aclImdb/imdb.vocab', encoding='utf8') as f:\n",
    "    #content = f.readlines()\n",
    "    universe_vocabulary = [x.strip() for x in f.readlines()]\n",
    "\n",
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "\n",
    "#Checking the not alphanumeric characters in vocabulary\n",
    "non_alphanumeric_set = set()\n",
    "for word in universe_vocabulary:\n",
    "    non_alphanumeric_set |= set(re.findall('\\W', word))\n",
    "print('Non alphanumeric characters found in universe vocabulary', non_alphanumeric_set)\n",
    "\n",
    "\n",
    "stripped_positive_tokenized = []\n",
    "for tokens in positive_tokenized:\n",
    "  stripped_positive_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_positive_tokenized]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "DSFWrZInMueS",
    "outputId": "4ae1eab3-3e09-41a5-9778-22aa72ce5cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count across all reviews (before stripping tokens): 269386\n",
      "Word count across all reviews (after stripping tokens): 223289\n"
     ]
    }
   ],
   "source": [
    "print(\"Word count across all reviews (before stripping tokens):\", sum([len(token) for token in positive_tokenized]))\n",
    "stripped_negative_tokenized = []\n",
    "for tokens in negative_tokenized:\n",
    "  stripped_negative_tokenized.append([token.lower() for token in tokens if token.lower() in universe_vocabulary])\n",
    "\n",
    "print(\"Word count across all reviews (after stripping tokens):\", sum([len(token) for token in stripped_negative_tokenized]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnu21deYOkDE"
   },
   "source": [
    "## Modelling \n",
    "\n",
    "We have decided to do the use the below models and vectorisation techniques to test our their accuracy / score, the idea is to use a one model and one vectorization technique and plot a score.\n",
    "\n",
    "**Simple models**\n",
    "\n",
    "- Logistic Regression\n",
    "- Random Forst\n",
    "- LSTM\n",
    "- GRU\n",
    "- CNN\n",
    "\n",
    "**Vectorisation techniques**\n",
    "- Bag of Words\n",
    "- TFIDF (weighted bag of words)\n",
    "- mapping words to plain integers\n",
    "- mapping words to embedding vectors\n",
    "\n",
    "** Embeddings to try\n",
    "- Word2Vec\n",
    "- FastText\n",
    "- Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mGh1Vqp3XNJI"
   },
   "source": [
    "## Logistic Regression.\n",
    "## Introducing Pipeline: http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "## Introducing TfdfVectorizer: http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "## Introducing cross_val_score http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "lfr3bXOgXNJJ",
    "outputId": "cc06dd0d-e886-4090-c972-cd05520adaf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive review(s): Way, way back in the 1980s, long before NAFTA was drafted and corporations began to shed their national identities, the United States and Japan were at each other's throat in the world manufacturing race. Remember sayings like 'Union Yes!,' 'the Japanese are taking this country over,' and 'Americans are lazy?'  As the Reagan era winded down and corporations edged towards a global marketplace, director Ron Howard made one of several trips into the comedy genre with his 1986 smash 'Gung Ho,' which drew over $36 million in U.S. box office receipts. While in many ways dated, Howard's tongue-in-cheek story of colliding cultures in the workplace still offers hard truth for industrial life today.  'Gung Ho' focuses on Hunt Stevenson (Michael Keaton), the automakers union rep from Hadleyville, a small, depressed town in the foothills of Pennsylvania. Stevenson has been asked to visit the Assan Motor Company in Tokyo (similar to real-life Toyota), which is considering a U.S. operation at the town's empty plant. With hundreds of residents out of work and the town verging on collapse, Assan decides to move in and Stevenson is hired as a liaison between company officials and workers on the assembly line.  The 112 minutes of 'Gung Ho' is a humorous look at these two sides, with their strengths and weaknesses equally considered: on one hand, an American workforce that values its traditions but is often caught in the frenzy of pride and trade unionism; on the other hand, Japanese workers who are extremely devoted to their job yet lacking in personal satisfaction and feelings of self-worth. In Stevenson, we find an American working class figure of average intelligence with the skills to chat people through misunderstandings. With the survival of his workers' jobs and most of Hadleyville on the line, Stevenson proves a likable guy who wants nothing more than a fair chance, although his cleverness will sink him into a great deal of trouble. Besides answering to the heads of Assan, we witness a delicate balancing act between Stevenson and his fellow union members, many of whom he grew up with. This includes Buster (George Wendt), Willie (John Turturro), and Paul (Clint Howard, Ron's brother).  The Japanese cast is headed by Gedde Watanabe, also known for 'Sixteen Candles' and 'Volunteers.' Watanabe plays Kazihiro, the plant manager who is down on his luck and begins to feel a sympathy for American life. He is constantly shadowed by Saito (Sab Shimono), the nephew of Assan's CEO who is desperate to take his spot in the pecking order. While given a light touch, these characters fare very well in conveying ideas of the Japanese working culture.  With Hunt Stevenson dominating the script, Michael Keaton has to give a solid performance for this film to work. 'Gung Ho' is indeed a slam-dunk success for Keaton, who also teamed with Ron Howard in 1994's 'The Paper.' He made this film during a string of lighter roles that included 'Mr. Mom,' 'Beetle Juice,' and 'The Dream Team' before venturing into 'Batman,' 'One Good Cop,' and 'My Life.' It's also hard not to like Gedde Watanabe's performance as the odd man out, who first wears Japanese ribbons of shame before teaming up with Stevenson to make the auto plant a cohesive unit.  The supporting cast is top-notch, including Wendt, Turturro, Shimono, and Soh Yamamura as Assan CEO Sakamoto. Mimi Rogers supplies a romantic interest as Audrey, Hunt's girlfriend. Edwin Blum, Lowell Ganz, and Babaloo Mandel teamed up for Gung Ho's solid writing. The incidental music, which received a BMI Film Music Award, was composed by Thomas Newman. Gung Ho's soundtrack songs are wall-to-wall 80s, including 'Don't Get Me Wrong,' 'Tuff Enuff,' and 'Working Class Man.'  The success of 'Gung Ho' actually led to a short-lived TV series on ABC. While more impressive as a social commentary twenty years ago, Ron Howard's film still has its comic value. It is available on DVD as part of the Paramount Widescreen Collection and is a tad short-changed. Audio options are provided in English 5.1 surround, English Dolby surround, and French 'dubbing,' but subtitles are in English only. There are no extras, not even the theatrical trailer. On the plus side, Paramount's digital transfer is quite good, with little grain after the opening credits and high quality sound. While a few extras would have been helpful - especially that 'Gung Ho' was a box office success - there's little to complain about the film presentation itself.  *** out of 4\n",
      "Negative review(s): I wish I'd known more about this movie when I rented it. I'd put it in my queue on the basis of Heather Graham and her strong cred as an actress (IMHO). While parts of the movie were charming, much of the movie felt contrived, undeveloped, or otherwise just boring or predictable. Not to mention the ICK factor of so many people thinking the sibs were a couple... I don't care how big a part of the story line that is, it still felt a bit, um, gross. And Charlie, for a zoologist, she certainly doesn't seem to be very attuned to signals from other Homo sapiens. What was it about her (besides her hotness and some common interests) that made Gray fall for her? The story could have been so much more interesting with a little more depth. High points - Molly Shannon (although I do agree with the reviewer who found her annoying on occasion), the cabbie in drag, and the dance sequences (if Sam & Gray were such great dancers, I wish we'd seen more of that, as the bits we were shown were indeed better than most of the rest of the movie). Could have been better.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_positives = pd.DataFrame({'reviews':[load_doc(x) for x in positive_sample_file_list], 'sentiment': np.ones(SAMPLE_SIZE)})\n",
    "df_negatives = pd.DataFrame({'reviews':[load_doc(x) for x in negative_sample_file_list], 'sentiment': np.zeros(SAMPLE_SIZE)})\n",
    "\n",
    "print(\"Positive review(s):\", df_positives['reviews'][1])\n",
    "print(\"Negative review(s):\", df_negatives['reviews'][1])\n",
    "\n",
    "df = pd.concat([df_positives, df_negatives], ignore_index=True)\n",
    "\n",
    "df = shuffle(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DIF9pizTcpWm"
   },
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of raws {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "# This is a sparse matrix \n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "# This is a  \n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "# In CountVec we have the vocabulary as an attribute\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "\n",
    "# In bag_of_words we have the vector features representing each single document \n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :])\n",
    "\n",
    "# Lets get our training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n",
    "\n",
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values{}'.format(len(results)/len(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-BsaFCwXNJQ"
   },
   "source": [
    "## Logistic Regress model using Bag of Words vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "-S-I4Fc2XNJS",
    "outputId": "7158b946-8734-47df-d4f4-9318b9d2a0c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/davepool/home/dns/.virtualenvs/ml-group/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 1.0\n",
      "Test accuracy 0.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/davepool/home/dns/.virtualenvs/ml-group/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy list [0.81333333 0.88       0.82666667 0.82666667 0.84       0.76666667\n",
      " 0.80666667 0.79333333 0.82       0.81333333] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/davepool/home/dns/.virtualenvs/ml-group/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy mean 0.8186666666666668 \n"
     ]
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "lr_CV = Pipeline([('vect', CountVec), ('clf', LogisticRegression(random_state=0))])\n",
    "lr_CV.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_CV.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_CV.score(X_test, y_test)))\n",
    "\n",
    "# # Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_CV = CountVec.fit_transform(X_train)\n",
    "type(X_train_CV)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_CV, y_train, cv= k_folds).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YeUltp6gXNJd"
   },
   "source": [
    "## Logistic Regress model using TfidfVectorizer vectorisation technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "s16mkhktXNJf",
    "outputId": "6fce5e0d-1d45-432e-e7cb-f1fd5972bc42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 0.9693333333333334\n",
      "Test accuracy 0.838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/davepool/home/dns/.virtualenvs/ml-group/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/davepool/home/dns/.virtualenvs/ml-group/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy list [0.80666667 0.84       0.81333333 0.84666667 0.81333333 0.75333333\n",
      " 0.79333333 0.79333333 0.81333333 0.82666667] \n",
      "Train accuracy mean 0.8099999999999999 \n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0))])\n",
    "lr_tfidf.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_tfidf.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_tfidf.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2lMqoGKXNJl"
   },
   "source": [
    "## Logistic Regress model using TfidfVectorizer and different values for C hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "zZZOXbA4XNJm",
    "outputId": "518f52ab-2ccd-4bca-9105-5fdcf5c797ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_value 1.0 Test Score 0.838 Train_score 0.9693333333333334\n",
      "C_value 1.1 Test Score 0.84 Train_score 0.9713333333333334\n",
      "C_value 1.2000000000000002 Test Score 0.84 Train_score 0.9733333333333334\n",
      "C_value 1.3000000000000003 Test Score 0.84 Train_score 0.9766666666666667\n",
      "C_value 1.4000000000000004 Test Score 0.838 Train_score 0.9813333333333333\n",
      "C_value 1.5000000000000004 Test Score 0.838 Train_score 0.9833333333333333\n",
      "C_value 1.6000000000000005 Test Score 0.842 Train_score 0.9846666666666667\n",
      "C_value 1.7000000000000006 Test Score 0.844 Train_score 0.9846666666666667\n",
      "C_value 1.8000000000000007 Test Score 0.844 Train_score 0.9866666666666667\n",
      "C_value 1.9000000000000008 Test Score 0.842 Train_score 0.9893333333333333\n"
     ]
    }
   ],
   "source": [
    "C_values = np.arange(1,2,0.1)\n",
    "results = []\n",
    "\n",
    "for value in C_values:   \n",
    "    lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(random_state=0, C=value))])\n",
    "    lr_tfidf.fit(X_train, y_train)\n",
    "    train_score = lr_tfidf.score(X_train, y_train)\n",
    "    score = lr_tfidf.score(X_test, y_test)\n",
    "    print('C_value {} Test Score {} Train_score {}'.format(value, score, train_score))\n",
    "    results.append(score)\n",
    "\n",
    "time_end_of_notebook = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "colab_type": "code",
    "id": "6siRLHQU79F7",
    "outputId": "dbc21ee2-1066-4b68-c8a0-ad47ca57f4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 1000\n",
      "Full notebook execution duration: 103.69539499282837 seconds\n",
      "Full notebook execution duration: 1.7282565832138062 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>Vectorisation techniques</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Pending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>TFIDF</td>\n",
       "      <td>0.989333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Models Vectorisation techniques     Score\n",
       "0  Logistic Regression             Bag of Words     0.842\n",
       "1  Logistic Regression                 Word2Vec   Pending\n",
       "2  Logistic Regression                    TFIDF  0.989333"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_models_vectorization = pd.DataFrame(\n",
    "     {'Models':                   [\"Logistic Regression\", \"Logistic Regression\", \"Logistic Regression\"], \n",
    "      'Vectorisation techniques': [\"Bag of Words\",        \"Word2Vec\", \"TFIDF\"], \n",
    "      'Score':                    [score,                 \"Pending\", lr_tfidf.score(X_train, y_train) ]},\n",
    "    columns=['Models','Vectorisation techniques','Score']\n",
    ")\n",
    "print(\"Sample size:\", SAMPLE_SIZE)\n",
    "\n",
    "duration = time_end_of_notebook - time_beginning_of_notebook\n",
    "\n",
    "print(\"Full notebook execution duration:\", duration, \"seconds\")\n",
    "print(\"Full notebook execution duration:\", duration / 60, \"minutes\")\n",
    "\n",
    "table_models_vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-28yfXCkgTe"
   },
   "source": [
    "**The below two code blocks replaces the original/inital BoW implementation using Scikit-learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "colab_type": "code",
    "id": "XtV8N-0zXNJ4",
    "outputId": "d76d5eea-9796-4eb7-a158-1f6ccdcd3db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      " Number of raws 2000 (documents) -- Number of columns 25092 (vocabulary) \n",
      "\n",
      "\n",
      " Type of bag_of_words <class 'scipy.sparse.csr.csr_matrix'> \n",
      "\n",
      "\n",
      " Sparsity 0.9946102144109676 \n",
      "\n",
      "\n",
      " Type of bag_of_words.toarray <class 'numpy.ndarray'> \n",
      "\n",
      "\n",
      " Type of CountVec.vocabulary <class 'dict'> \n",
      "\n",
      "A sample of CountVec.vocabulary_ [('2005', 181), ('1890', 69), ('1912', 80), ('admirer', 590), ('1908', 77), ('abandonment', 346), ('43', 258), ('aforementioned', 698), ('admirably', 585), ('100th', 11), ('acquitted', 501), ('1990s', 161), ('1982', 152), ('allan', 882), ('75', 297), ('1968', 135), ('adieu', 573), ('alkie', 879), ('adhering', 572), ('abused', 411), ('alfredo', 859), ('2006', 182), ('400', 253), ('amato', 965), ('amerian', 998), ('ambient', 981), ('advancing', 633), ('adaptation', 540), ('acute', 530), ('acutely', 531), ('adams', 537), ('alvin', 956), ('14', 33), ('2003', 179), ('1998', 169), ('1800s', 51), ('allergic', 890), ('amateur', 961), ('35mm', 241), ('aggressive', 731), ('1981', 151), ('alpha', 928), ('adulterous', 624), ('akki', 804), ('adverse', 645), ('aircraft', 781), ('adam', 535), ('1890s', 70), ('actress', 522), ('advent', 637), ('alison', 877), ('accomplices', 445), ('amazing', 972), ('allowing', 904), ('60s', 282), ('abhorrent', 361), ('alcatraz', 834), ('1991', 162), ('aleisa', 846), ('1947', 112), ('acerbic', 471), ('adolescence', 603), ('absence', 394), ('1945', 110), ('accent', 420), ('agencies', 720), ('abound', 383), ('absorbing', 401), ('advani', 634), ('abject', 367), ('accomplishes', 447), ('adequate', 568), ('afghanistan', 691), ('again', 715), ('admittedly', 599), ('addiction', 551), ('2000s', 176), ('admission', 594), ('1915', 83), ('149', 35), ('7th', 303), ('activist', 516), ('allies', 896), ('addressed', 560), ('1943', 108), ('1865', 66), ('administer', 581), ('13th', 32), ('1925', 89), ('adolescent', 604), ('affects', 674), ('1837', 60), ('adjusted', 578), ('747', 296), ('adverts', 651), ('3500', 239), ('adopts', 611), ('airlift', 789), ('aids', 766), ('1933', 97), ('acosta', 494), ('altogether', 951), ('65', 284), ('alive', 878), ('aja', 798), ('125m', 26), ('1928', 91), ('22', 203), ('acorn', 493), ('33', 236), ('alex', 851), ('actually', 527), ('50', 267), ('abc', 352), ('alphonse', 930), ('2020', 189), ('active', 513), ('alexander', 853), ('ahem', 754), ('accepting', 428), ('alonzo', 922), ('30min', 230), ('adeptly', 567), ('3k', 250), ('agnes', 737), ('alladin', 881), ('abruptly', 392), ('actualy', 528), ('agenda', 722), ('1950', 115), ('1978', 147), ('aldonova', 843), ('aged', 719), ('alesia', 849), ('66er', 286), ('admires', 592), ('58', 277), ('amazed', 969), ('alumni', 954), ('aiden', 764), ('911', 321), ('aiming', 773), ('1900s', 75), ('aamir', 336), ('activism', 515), ('1967', 134), ('admiral', 586), ('40s', 255), ('18', 49), ('alligator', 897), ('accustomed', 469), ('admired', 589), ('accusing', 468), ('aborigines', 379), ('1980', 149), ('acid', 481), ('2002', 178), ('alaska', 823), ('abandoning', 345), ('afican', 693), ('alexa', 852), ('alice', 865), ('abandoned', 344), ('aggie', 729), ('06', 6), ('52', 272), ('accompany', 443), ('airplane', 792), ('afterthought', 711), ('1961', 128), ('01', 2), ('allure', 908), ('1700s', 47), ('am', 958), ('aardman', 338), ('adversaries', 643), ('accept', 424), ('affords', 688), ('amateurs', 964), ('05', 4), ('90', 318), ('affirm', 678), ('alarmingly', 820), ('accelerate', 419), ('acin', 483), ('ambulance', 991), ('abhorrence', 360), ('akshay', 805), ('45', 260), ('acquire', 498), ('able', 368), ('1962', 129), ('afrika', 704), ('advised', 655), ('alterations', 939), ('1932', 96), ('alantis', 817), ('2nd', 226), ('aborigine', 378), ('aims', 776), ('6am', 288), ('age', 718), ('afford', 686), ('1942', 107), ('advice', 652), ('agreed', 746), ('addictions', 552), ('achingly', 480), ('700', 290), ('25th', 214), ('34th', 237), ('acl', 489), ('alleviate', 891), ('1200f', 24), ('akira', 802), ('alcoholic', 837), ('aesthetic', 662), ('amanda', 960), ('abs', 393), ('allowances', 902), ('alahani', 812), ('albeit', 826), ('affairs', 667), ('accidental', 434), ('abusing', 413), ('abominations', 375), ('1812', 54), ('align', 873), ('adrift', 620), ('abnormal', 370), ('agatha', 717), ('108', 15), ('actresses', 523), ('accompanied', 440), ('adjective', 575), ('1929', 92), ('21', 200), ('2023', 190), ('237', 205), ('ago', 738), ('1940', 104), ('1970', 138), ('27', 218), ('1860', 63), ('17million', 48), ('aficionados', 695), ('adoration', 613), ('alphabet', 929), ('ahhhh', 756), ('aluminum', 953), ('absurdist', 405), ('9pm', 330), ('ambulances', 992), ('alarms', 821), ('abo', 371), ('afterward', 712), ('aesthetics', 663), ('acre', 502), ('afraid', 700), ('95', 325), ('alterio', 942), ('300', 228), ('accepts', 429), ('ambiguity', 984), ('acme', 491), ('abbey', 349), ('acres', 503), ('aficionado', 694), ('accentuates', 423), ('abyss', 416), ('adrenaline', 617), ('7days', 302), ('admittance', 597), ('97', 328), ('aloha', 915), ('ain', 778), ('alluded', 907), ('alert', 848), ('adjurdubois', 576), ('ailing', 768), ('amature', 967), ('17', 46), ('alien', 868), ('admiring', 593), ('abomination', 374), ('alone', 918), ('amaze', 968), ('accurate', 461), ('acrobatics', 505), ('1995', 166), ('afterwards', 713), ('alarm', 818), ('actor', 520), ('1980s', 150), ('aired', 782), ('adept', 566), ('accountable', 454), ('afloat', 696), ('80s', 305), ('78', 300), ('1930', 93), ('amateurish', 962), ('actions', 512), ('affinity', 677), ('advanced', 629), ('adapted', 542), ('allegedly', 885), ('after', 706), ('82', 306), ('1935', 99), ('amazement', 970), ('affirmations', 679), ('addams', 547), ('acknowledged', 487), ('alley', 892), ('aching', 479), ('alohalani', 916), ('15', 38), ('1992', 163), ('137', 30), ('affair', 666), ('alliances', 894), ('africa', 701), ('1927', 90), ('_by', 333), ('alan', 815), ('alamo', 814), ('aloud', 926), ('16s', 44), ('afflicted', 684), ('allotted', 900), ('alexei', 856), ('ambles', 990), ('1922', 88), ('alike', 875), ('ably', 369), ('achievement', 475), ('accounted', 456), ('4o', 265), ('akin', 801), ('adobe', 602), ('agrees', 749), ('14s', 36), ('85', 310), ('136', 29), ('accountant', 455), ('akshaye', 807), ('aluminium', 952), ('aldo', 842), ('adapting', 543), ('alas', 822), ('2080', 195), ('affirmed', 681), ('action', 510), ('ackland', 485), ('adventurous', 641), ('afore', 697), ('aires', 783), ('alfrie', 860), ('alps', 931), ('1960', 126), ('alida', 867), ('adjust', 577), ('afficionado', 675), ('amazonian', 975), ('140', 34), ('alain', 813), ('adele', 564), ('ahold', 758), ('accomplishment', 448), ('aloof', 923), ('absurdly', 407), ('advance', 628), ('1814', 56), ('airless', 788), ('alois', 917), ('abominator', 376), ('allegiance', 886), ('affect', 668), ('allegory', 888), ('24m30s', 210), ('1986', 156), ('alot', 924), ('acquainted', 497), ('administration', 582), ('acclaimed', 438), ('adopter', 609), ('70s', 292), ('29th', 222), ('absurdity', 406), ('000', 1), ('94', 323), ('190', 73), ('all', 880), ('addressing', 562), ('adopt', 607), ('amazingly', 973), ('adjusts', 579), ('1976', 145), ('aghast', 733), ('_am_', 332), ('abe', 356), ('1990', 160), ('1920s', 87), ('although', 949), ('1979', 148), ('16th', 45), ('affected', 670), ('acolyte', 492), ('42', 256), ('accidents', 436), ('amenable', 995), ('affections', 673), ('agents', 726), ('89', 314), ('2hours', 224), ('admit', 595), ('allow', 901), ('amber', 977), ('acceptable', 425), ('alternated', 944), ('affluent', 685), ('ambition', 986), ('abrasive', 389), ('3000', 229), ('1910s', 79), ('agonizingly', 740), ('afi', 692), ('adaption', 544), ('ages', 728), ('affability', 664), ('alicia', 866), ('1973', 142), ('adultery', 625), ('alarming', 819), ('accusations', 464), ('ambiguous', 985), ('accumulator', 459), ('altered', 940), ('airplanes', 793), ('92', 322), ('adhere', 570), ('1820', 57), ('abducted', 354), ('admiration', 587), ('73', 293), ('always', 957), ('alothugh', 925), ('admits', 596), ('1964', 131), ('abetted', 358), ('absolutely', 397), ('701', 291), ('16', 40), ('against', 716), ('ace', 470), ('1st', 172), ('ambassador', 976), ('adaptations', 541), ('ambiguities', 983), ('aankhen', 337), ('add', 546), ('abbott', 351), ('8p', 316), ('abdic', 353), ('110', 17), ('ahab', 752), ('air', 779), ('ajay', 799), ('197', 137), ('africans', 703), ('alloted', 899), ('250', 212), ('aimée', 777), ('alabama', 810), ('amenábar', 997), ('acquaintance', 495), ('47', 262), ('actors', 521), ('alta', 935), ('____', 331), ('also', 934), ('alexandra', 854), ('ally', 911), ('acidic', 482), ('54', 274), ('alanis', 816), ('abu', 408), ('180d', 53), ('abominable', 373), ('1987', 157), ('18137', 55), ('abbot', 350), ('adelle', 565), ('51', 271), ('32', 235), ('allows', 905), ('actuelly', 529), ('addict', 549), ('amends', 996), ('120', 23), ('african', 702), ('accepted', 427), ('37', 244), ('alibi', 864), ('acquired', 499), ('53', 273), ('airborne', 780), ('ab', 342), ('albright', 830), ('adherence', 571), ('adequately', 569), ('america', 999), ('aisles', 797), ('112', 20), ('8th', 317), ('19', 72), ('1941', 106), ('abide', 362), ('ai', 760), ('amazes', 971), ('105', 14), ('adolf', 606), ('affleck', 682), ('altair', 936), ('accounts', 457), ('adding', 554), ('already', 932), ('2019', 188), ('1946', 111), ('affirmative', 680), ('210', 201), ('absorption', 402), ('aaron', 339), ('alters', 948), ('absolute', 396), ('20th', 199), ('albums', 832), ('activists', 517), ('ambience', 980), ('1937', 101), ('42nd', 257), ('adjani', 574), ('alcoholics', 838), ('administrators', 583), ('1916', 84), ('agreeably', 745), ('2600', 216), ('acting', 509), ('admirable', 584), ('addictive', 553), ('8700', 312), ('achieving', 478), ('1963', 130), ('1840s', 62), ('1993', 164), ('according', 451), ('afghan', 690), ('amc', 993), ('1994', 165), ('accessible', 432), ('accused', 466), ('1840', 61), ('agendas', 723), ('1969', 136), ('30something', 233), ('160', 41), ('alvarez', 955), ('accident', 433), ('altar', 937), ('advisable', 653), ('aboriginal', 377), ('adds', 563), ('aimed', 772), ('adama', 536), ('1989', 159), ('amalgamation', 959), ('ah', 751), ('1100', 18), ('aftermath', 708), ('ala', 809), ('150', 39), ('accessibility', 431), ('abounds', 384), ('addition', 555), ('1950s', 116), ('affection', 672), ('aiello', 767), ('airheadedness', 786), ('adventures', 639), ('airways', 795), ('ads', 621), ('2001', 177), ('admirers', 591), ('alecia', 845), ('adversary', 644), ('accuracy', 460), ('adventure', 638), ('1824', 58), ('204', 191), ('address', 559), ('abortions', 382), ('adolescents', 605), ('acts', 524), ('60', 279), ('albert', 828), ('advantages', 636), ('2090', 196), ('44', 259), ('acquaintances', 496), ('12th', 27), ('allegorical', 887), ('1956', 122), ('200', 174), ('akiva', 803), ('1600s', 42), ('aisle', 796), ('83', 308), ('1955', 121), ('adulthood', 626), ('adorable', 612), ('alimony', 876), ('agent', 724), ('almighty', 912), ('allied', 895), ('2070', 194), ('1952', 118), ('88', 313), ('abdullah', 355), ('102', 13), ('aciton', 484), ('600', 280), ('1805', 52), ('afternoon', 709), ('1800', 50), ('1949', 114), ('ahmed', 757), ('1939', 103), ('agreeing', 747), ('aida', 762), ('20m', 197), ('1997', 168), ('ali', 862), ('amble', 989), ('1999', 170), ('algerian', 861), ('24', 207), ('acknowledge', 486), ('across', 506), ('1931', 95), ('acquiring', 500), ('accented', 421), ('1960s', 127), ('advancements', 631), ('35', 238), ('adored', 615), ('1918', 85), ('31', 234), ('adler', 580), ('ahead', 753), ('abandon', 343), ('aimlessly', 775), ('11th', 21), ('55', 275), ('2000', 175), ('accompanies', 441), ('airline', 790), ('accompaniment', 442), ('38', 245), ('96', 327), ('agentine', 725), ('achieved', 474), ('alcohol', 836), ('02', 3), ('affiliation', 676), ('125', 25), ('alexandre', 855), ('1965', 132), ('aauugghh', 341), ('ailments', 769), ('alessandra', 850), ('adapt', 539), ('aboard', 372), ('1971', 140), ('94th', 324), ('acclaim', 437), ('airport', 794), ('6000', 281), ('14th', 37), ('alonso', 921), ('adviser', 656), ('alcoholism', 839), ('absent', 395), ('alastair', 824), ('26', 215), ('2004', 180), ('25', 211), ('1975', 144), ('1870s', 67), ('alright', 933), ('1873', 68), ('alabaster', 811), ('agricultural', 750), ('77', 299), ('alba', 825), ('acrid', 504), ('30s', 232), ('13', 28), ('86', 311), ('5th', 278), ('1974', 143), ('actively', 514), ('1862', 65), ('19th', 171), ('alda', 840), ('8mm', 315), ('alejandro', 847), ('adriatic', 619), ('access', 430), ('advances', 632), ('adversity', 646), ('airheaded', 785), ('2008', 184), ('ambidexterous', 979), ('advises', 657), ('250000', 213), ('ager', 727), ('1984', 154), ('absorbed', 400), ('80', 304), ('adrian', 618), ('accuses', 467), ('abroad', 390), ('advertising', 650), ('74', 295), ('2400', 208), ('84', 309), ('11', 16), ('1972', 141), ('accrued', 458), ('56', 276), ('2009', 185), ('100', 9), ('30pm', 231), ('4000', 254), ('aime', 771), ('ambigious', 982), ('79', 301), ('addicted', 550), ('1913', 81), ('aborted', 380), ('1900', 74), ('1906', 76), ('ached', 472), ('acted', 508), ('ahh', 755), ('abusive', 414), ('1985', 155), ('abusers', 412), ('achieve', 473), ('afternoons', 710), ('1934', 98), ('36', 243), ('alden', 841), ('alberto', 829), ('alternative', 946), ('achievements', 476), ('adult', 622), ('alleged', 884), ('40', 252), ('actual', 525), ('alienate', 869), ('admittingly', 600), ('275', 219), ('agree', 743), ('actuality', 526), ('affable', 665), ('4500', 261), ('advantage', 635), ('1940s', 105), ('480p', 264), ('99', 329), ('alexis', 857), ('183', 59), ('alec', 844), ('20', 173), ('1970s', 139), ('alluring', 909), ('accusatory', 465), ('2007', 183), ('almost', 914), ('admitted', 598), ('2480', 209), ('agoraphobic', 742), ('allude', 906), ('advertisement', 648), ('accosted', 452), ('allusions', 910), ('500', 268), ('added', 548), ('agency', 721), ('abating', 347), ('1951', 117), ('2010', 186), ('advise', 654), ('alfred', 858), ('alternate', 943), ('ae', 661), ('1920', 86), ('agreeable', 744), ('alias', 863), ('750', 298), ('almodovar', 913), ('afterlife', 707), ('alienation', 871), ('aim', 770), ('68', 287), ('adorned', 616), ('al', 808), ('amateuristic', 963), ('adults', 627), ('64', 283), ('ambiance', 978), ('activities', 518), ('2d', 223), ('3rd', 251), ('09', 7), ('abortion', 381), ('aides', 765), ('abel', 357), ('70', 289), ('1996', 167), ('aimless', 774), ('additional', 556), ('1930s', 94), ('23', 204), ('academy', 418), ('18th', 71), ('alchemy', 835), ('ag', 714), ('about', 385), ('50s', 270), ('1944', 109), ('aided', 763), ('10', 8), ('activity', 519), ('ada', 533), ('alija', 874), ('ahoy', 759), ('adapts', 545), ('2060', 193), ('adventuresome', 640), ('1948', 113), ('alliance', 893), ('35pm', 242), ('amatuer', 966), ('abhay', 359), ('adore', 614), ('00', 0), ('29', 221), ('950', 326), ('aforesaid', 699), ('aggrivating', 732), ('abundant', 409), ('1000', 10), ('accordance', 450), ('1959', 125), ('accommodate', 439), ('30', 227), ('ac', 417), ('altman', 950), ('101', 12), ('21st', 202), ('12', 22), ('1954', 120), ('1914', 82), ('adulteries', 623), ('1977', 146), ('3am', 248), ('absolution', 398), ('adabted', 534), ('ad', 532), ('abstract', 403), ('adopted', 608), ('afganistan', 689), ('66', 285), ('addresses', 561), ('1909', 78), ('16mm', 43), ('aarrrgh', 340), ('1966', 133), ('advisor', 659), ('5000', 269), ('allison', 898), ('23rd', 206), ('39', 247), ('airing', 787), ('90s', 320), ('advision', 658), ('2047', 192), ('additionally', 557), ('albuquerque', 833), ('advocate', 660), ('agonizing', 739), ('agony', 741), ('alongside', 920), ('aachen', 335), ('abrahams', 388), ('amazon', 974), ('advertises', 649), ('album', 831), ('alter', 938), ('abraham', 387), ('aid', 761), ('agitated', 735), ('1100ad', 19), ('account', 453), ('alternatively', 947), ('26th', 217), ('allen', 889), ('1861', 64), ('28', 220), ('altering', 941), ('90210', 319), ('2012', 187), ('actioned', 511), ('along', 919), ('accidentally', 435), ('1953', 119), ('1983', 153), ('afforded', 687), ('8230', 307), ('advancement', 630), ('aging', 734), ('absurd', 404), ('1938', 102), ('1988', 158), ('akshaya', 806), ('4th', 266), ('additions', 558), ('accomplice', 444), ('139', 31), ('absorb', 399), ('abysmal', 415), ('airfield', 784), ('adventurousness', 642), ('abrupt', 391), ('admire', 588), ('abilityof', 366), ('ado', 601), ('above', 386), ('1957', 123), ('afro', 705), ('agreement', 748), ('1958', 124), ('allowed', 903), ('1936', 100), ('abbas', 348), ('alternates', 945), ('_could', 334), ('7300', 294), ('agitator', 736), ('35mins', 240), ('abuse', 410), ('airlines', 791), ('ameche', 994), ('adamson', 538), ('allegations', 883), ('2hr', 225), ('abilities', 364), ('aggression', 730), ('accusation', 463), ('aclu', 490), ('accomplished', 446), ('accord', 449), ('albeniz', 827), ('advertised', 647), ('afflict', 683), ('aka', 800), ('achieves', 477), ('alienating', 870), ('accurately', 462), ('aloysius', 927), ('accents', 422), ('ambitions', 987), ('48', 263), ('38th', 246), ('act', 507), ('affecting', 671), ('ambitious', 988), ('20s', 198), ('ability', 365), ('050', 5), ('3d', 249), ('affectation', 669), ('acceptance', 426), ('abiding', 363), ('acknowledgement', 488), ('aliens', 872), ('adoptive', 610)]\n",
      "\n",
      " Type of bag_of_words.toarray() {} \n",
      "\n",
      "\n",
      " First feature vector, representing the first document \n",
      " \n"
     ]
    }
   ],
   "source": [
    "CountVec = CountVectorizer()\n",
    "# Creating the BoW with the set of all the documents and transforming the documents in feature vectors\n",
    "bag_of_words = CountVec.fit_transform(df['reviews'])\n",
    "\n",
    "print(type(bag_of_words))\n",
    "print('\\n Number of raws {} (documents) -- Number of columns {} (vocabulary) \\n'.format(bag_of_words.shape[0], bag_of_words.shape[1]))\n",
    "\n",
    "# https://machinelearningmastery.com/sparse-matrices-for-machine-learning/\n",
    "# This is a sparse matrix \n",
    "print('\\n Type of bag_of_words {} \\n'.format(type(bag_of_words)))\n",
    "sparsity = 1.0 - bag_of_words.nnz / (bag_of_words.shape[0] * bag_of_words.shape[1])\n",
    "print('\\n Sparsity {} \\n'.format(sparsity))\n",
    "\n",
    "# This is a  \n",
    "print('\\n Type of bag_of_words.toarray {} \\n'.format(type(bag_of_words.toarray())))\n",
    "\n",
    "# In CountVec we have the vocabulary as an attribute\n",
    "print('\\n Type of CountVec.vocabulary {} \\n'.format(type(CountVec.vocabulary_)))\n",
    "print('A sample of CountVec.vocabulary_ {}'.format([(k, v) for k, v in CountVec.vocabulary_.items() if v < 1000]))\n",
    "\n",
    "# In bag_of_words we have the vector features representing each single document \n",
    "print('\\n Type of bag_of_words.toarray() {} \\n')\n",
    "print('\\n First feature vector, representing the first document \\n', bag_of_words[0, :2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6A2pKzOmxzT"
   },
   "outputs": [],
   "source": [
    "# Lets get our training and test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(bag_of_words.toarray(), df['sentiment'].values, test_size=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "pqbPNSGUg4Rj",
    "outputId": "b5976680-a41d-4736-99b7-e5e17ce473f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using score function: 0.814\n",
      "Percentage of correct predicted values: 0.814\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Using score function: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values: {}'.format(len(results)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0YIRBaroMeM"
   },
   "source": [
    "### RandomForestClassifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "nMUUsMLkkyLO",
    "outputId": "8b2096de-ded8-4e02-b75a-046b1861f529"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using score function: 0.822\n",
      "Percentage of correct predicted values: 0.822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=1000)\n",
    "clf.fit(X_train, y_train)\n",
    "print('Using score function: {}'.format(clf.score(X_test, y_test)))\n",
    "\n",
    "\n",
    "results = [(predicted, actual) for predicted, actual in zip(clf.predict(X_test),  y_test) \n",
    "           if  predicted == actual]\n",
    "\n",
    "print('Percentage of correct predicted values: {}'.format(len(results)/len(y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM0YI71dnpZE"
   },
   "source": [
    "### RandomForestClassifer with TFIDF (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "HNluLDB4nayq",
    "outputId": "2ee1d453-891e-45d9-ad20-2d8bd81c3853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy 1.0\n",
      "Test accuracy 0.826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/davepool/home/dns/.virtualenvs/ml-group/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/davepool/home/dns/.virtualenvs/ml-group/lib/python3.5/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy list [0.82781457 0.78145695 0.82666667 0.81333333 0.72666667 0.9\n",
      " 0.88       0.80666667 0.82550336 0.83892617] \n",
      "Train accuracy mean 0.8227034386713484 \n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', RandomForestClassifier(n_estimators=1000))])\n",
    "lr_tfidf.fit(X_train, y_train)\n",
    "print('Train accuracy {}'.format(lr_tfidf.score(X_train, y_train)))\n",
    "print('Test accuracy {}'.format(lr_tfidf.score(X_test, y_test)))\n",
    "\n",
    "# Trying with cross_val_score\n",
    "lr = LogisticRegression()\n",
    "k_folds = 10\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "print('Train accuracy list {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds))) \n",
    "print('Train accuracy mean {} '.format(cross_val_score(lr, X_train_tfidf, y_train, cv= k_folds).mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fjDcOMEUoToE"
   },
   "source": [
    "### LSTM with Keras (Sequential model)\n",
    "\n",
    "\n",
    "Please note that the below code is executed on GPU instances on Colab, this wont work on your local machine, use the flag to enable/disable running in CPU or GPU mode, set `run_in_GPU_mode_on_colab=false` in order to be able to run in CPU mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "fVN5C_34oSgO",
    "outputId": "03f65fd1-c9fb-47dc-8025-afa10d4f68d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:26: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1350 samples, validate on 150 samples\n",
      "Epoch 1/2\n",
      "1350/1350 [==============================] - 62s 46ms/step - loss: 0.6947 - acc: 0.4822 - val_loss: 0.6932 - val_acc: 0.4933\n",
      "Epoch 2/2\n",
      "1350/1350 [==============================] - 59s 43ms/step - loss: 0.6934 - acc: 0.4919 - val_loss: 0.6928 - val_acc: 0.5333\n",
      "500/500 [==============================] - 5s 10ms/step\n",
      "Test score: 0.6931333312988281\n",
      "Test accuracy: 0.4959999995231628\n",
      "Duration on the CPU: 128.34940028190613\n"
     ]
    }
   ],
   "source": [
    "# NOTE this is broken - tokenisation is incorrect. Will fix in next update\n",
    "import tensorflow as tf\n",
    "\n",
    "def lstm_keras():\n",
    "  from keras.models import Sequential\n",
    "  from keras.layers import Dense, Activation, Embedding, LSTM\n",
    "  from keras.preprocessing.text import Tokenizer\n",
    "  from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(df['reviews'], df['sentiment'], test_size=0.25)\n",
    "\n",
    "  vocab_size = 1000\n",
    "  tokenize = Tokenizer(num_words=vocab_size)\n",
    "  tokenize.fit_on_texts(X_train)\n",
    "\n",
    "  encoded_X_train = tokenize.texts_to_matrix(X_train)\n",
    "  encoded_X_test = tokenize.texts_to_matrix(X_test)\n",
    "\n",
    "  encoder = LabelBinarizer()\n",
    "  encoder.fit(y_train)\n",
    "  encoded_y_train = encoder.transform(y_train)\n",
    "  encoded_y_test = encoder.transform(y_test)\n",
    "\n",
    "  max_features = 1000\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(max_features, 128, dropout=0.2))\n",
    "  model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))  # try using a GRU instead, for fun\n",
    "  model.add(Dense(1))\n",
    "  model.add(Activation('sigmoid'))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',\n",
    "               optimizer='adam',\n",
    "               metrics=['accuracy'])\n",
    "\n",
    "  batch_size=64\n",
    "  epochs=10\n",
    "  history = model.fit(encoded_X_train, encoded_y_train, \n",
    "                  batch_size=batch_size, \n",
    "                  epochs=epochs, \n",
    "                  verbose=1, \n",
    "                  validation_split=0.1)\n",
    "\n",
    "  score = model.evaluate(encoded_X_test, encoded_y_test, \n",
    "                         batch_size=batch_size, verbose=1)\n",
    "  print('Test score:', score[0])\n",
    "  print('Test accuracy:', score[1])\n",
    "\n",
    "\n",
    "run_in_GPU_mode_on_colab=False\n",
    "\n",
    "if run_in_GPU_mode_on_colab:  \n",
    "  config = tf.ConfigProto()\n",
    "  config.gpu_options.allow_growth = True\n",
    "\n",
    "  with tf.device('/gpu:0'):\n",
    "    session_gpu = tf.Session(config=config)\n",
    "    session_gpu.run(tf.global_variables_initializer())\n",
    "    session_gpu.run(tf.tables_initializer())\n",
    "    start = time.time()\n",
    "    session_gpu.run(lstm_keras())\n",
    "    end = time.time()\n",
    "    gpu_time = end - start\n",
    "    print('Duration on the GPU: {} seconds'.format(gpu_time))\n",
    "else:\n",
    "  start = time.time()\n",
    "  lstm_keras()\n",
    "  end = time.time()\n",
    "  cpu_time = end - start\n",
    "  print('Duration on the CPU: {} seconds'.format(cpu_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- dense neural network with bag of words\n",
    "- dense neural network with fixed size input\n",
    "- LSTM\n",
    "- CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gvChJ9kdot4q"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Sentiment analysis of movies (IMDB).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
